{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LLM Training Scripts","text":"<p>Tools and training scripts I have developed for building large language models in PyTorch.</p> <p>This repository provides:</p> <ul> <li>data preprocessing scripts,</li> <li>training scripts, and</li> <li>training guides.</li> </ul> <p>This repository is the successor to my old training tools BERT-PyTorch as the old code had a lot of technical debt and was not well tested. Compared to the old repository, this codebase aims to have better code health and maintainability thanks to tests, type checking, linters, documentation, etc.</p>"},{"location":"#install","title":"Install","text":"<p>See the Installation Guide.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>See the available Guides.</p>"},{"location":"known-issues/","title":"Known Issues","text":"<p>There are no known issues at the time. If you encounter a problem, consider opening an issue.</p>"},{"location":"api/","title":"llm","text":"<code>llm/__init__.py</code> <p>LLM package.</p> <p>Large language model training tools. Preprocessing scripts are provided in <code>llm.preprocess</code>, and training scripts in <code>llm.trainers</code>.</p>"},{"location":"api/SUMMARY/","title":"SUMMARY","text":"<ul> <li>llm<ul> <li>llm.checkpoint</li> <li>llm.config</li> <li>llm.datasets<ul> <li>llm.datasets.bert</li> <li>llm.datasets.roberta</li> <li>llm.datasets.sharded</li> </ul> </li> <li>llm.engine<ul> <li>llm.engine.accumulation</li> <li>llm.engine.amp</li> <li>llm.engine.base</li> <li>llm.engine.initialize</li> </ul> </li> <li>llm.environment</li> <li>llm.initialize</li> <li>llm.kfac</li> <li>llm.loss</li> <li>llm.models<ul> <li>llm.models.bert</li> </ul> </li> <li>llm.optimizers</li> <li>llm.preprocess<ul> <li>llm.preprocess.download</li> <li>llm.preprocess.format</li> <li>llm.preprocess.roberta</li> <li>llm.preprocess.shard</li> <li>llm.preprocess.tokenizer</li> <li>llm.preprocess.utils</li> </ul> </li> <li>llm.schedulers</li> <li>llm.timer</li> <li>llm.trainers<ul> <li>llm.trainers.bert<ul> <li>llm.trainers.bert.convert</li> <li>llm.trainers.bert.data</li> <li>llm.trainers.bert.main</li> <li>llm.trainers.bert.utils</li> </ul> </li> <li>llm.trainers.gpt<ul> <li>llm.trainers.gpt.arguments</li> <li>llm.trainers.gpt.data</li> <li>llm.trainers.gpt.main</li> <li>llm.trainers.gpt.model</li> <li>llm.trainers.gpt.optimizer</li> </ul> </li> </ul> </li> <li>llm.utils</li> </ul> </li> </ul>"},{"location":"api/checkpoint/","title":"llm.checkpoint","text":"<code>llm/checkpoint.py</code>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint","title":"Checkpoint","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Data loaded from a checkpoint.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.filepath","title":"filepath  <code>instance-attribute</code>","text":"<pre><code>filepath: str\n</code></pre> <p>Filepath of checkpoint.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.global_step","title":"global_step  <code>instance-attribute</code>","text":"<pre><code>global_step: int\n</code></pre> <p>Global step of checkpoint.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.model_state_dict","title":"model_state_dict  <code>instance-attribute</code>","text":"<pre><code>model_state_dict: dict[Any, Any]\n</code></pre> <p>Model state dictionary.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.optimizer_state_dict","title":"optimizer_state_dict  <code>instance-attribute</code>","text":"<pre><code>optimizer_state_dict: dict[Any, Any] | None\n</code></pre> <p>Optimizer state dictionary.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.scheduler_state_dict","title":"scheduler_state_dict  <code>instance-attribute</code>","text":"<pre><code>scheduler_state_dict: dict[Any, Any] | None\n</code></pre> <p>Scheduler state dictionary.</p>"},{"location":"api/checkpoint/#llm.checkpoint.Checkpoint.kwargs","title":"kwargs  <code>instance-attribute</code>","text":"<pre><code>kwargs: dict[str, Any]\n</code></pre> <p>Additional keyword arguments stored in the checkpoint.</p>"},{"location":"api/checkpoint/#llm.checkpoint.load_checkpoint","title":"load_checkpoint","text":"<pre><code>load_checkpoint(\n    checkpoint_dir: str | Path,\n    global_step: int | None = None,\n    map_location: Any = None,\n) -&gt; Checkpoint | None\n</code></pre> <p>Load checkpoint from directory.</p> <p>Parameters:</p> <ul> <li> <code>checkpoint_dir</code>               (<code>str | Path</code>)           \u2013            <p>Directory containing checkpoint files.</p> </li> <li> <code>global_step</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Global step checkpoint to load. If <code>None</code>, loads the latest checkpoint.</p> </li> <li> <code>map_location</code>               (<code>Any</code>, default:                   <code>None</code> )           \u2013            <p>Optional map_location to pass to <code>torch.load()</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Checkpoint | None</code>           \u2013            <p>Checkpoint or <code>None</code> if no checkpoint was found.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>OSError</code>             \u2013            <p>If <code>checkpoint_dir</code> does not exist.</p> </li> <li> <code>OSError</code>             \u2013            <p>If <code>global_step</code> is specified but the file does not exist.</p> </li> </ul> Source code in <code>llm/checkpoint.py</code> <pre><code>def load_checkpoint(\n    checkpoint_dir: str | pathlib.Path,\n    global_step: int | None = None,\n    map_location: Any = None,\n) -&gt; Checkpoint | None:\n    \"\"\"Load checkpoint from directory.\n\n    Args:\n        checkpoint_dir: Directory containing checkpoint files.\n        global_step: Global step checkpoint to load. If `None`,\n            loads the latest checkpoint.\n        map_location: Optional map_location to pass to\n            [`torch.load()`][torch.load].\n\n    Returns:\n        Checkpoint or `None` if no checkpoint was found.\n\n    Raises:\n        OSError: If `checkpoint_dir` does not exist.\n        OSError: If `global_step` is specified but the file does not exist.\n    \"\"\"\n    dir_path = pathlib.Path(checkpoint_dir)\n    if not dir_path.is_dir():\n        raise OSError(f'Checkpoint directory {checkpoint_dir} does not exist.')\n\n    if global_step is None:\n        checkpoints = {\n            match.group(1): str(p)\n            for p in dir_path.iterdir()\n            if (match := CHECKPOINT_NAME_RE.fullmatch(p.name)) is not None\n        }\n        if len(checkpoints) == 0:\n            return None\n        steps = [int(key) for key in checkpoints]\n        global_step = max(steps)\n\n    assert global_step is not None\n\n    checkpoint_path = dir_path / f'global_step_{global_step}.pt'\n    if not checkpoint_path.is_file():\n        raise OSError(f'Checkpoint named {checkpoint_path} does not exist.')\n\n    state_dict = torch.load(checkpoint_path, map_location=map_location)\n\n    model_state_dict = state_dict.pop('model')\n    optimizer_state_dict = state_dict.pop('optimizer', None)\n    scheduler_state_dict = state_dict.pop('scheduler', None)\n\n    return Checkpoint(\n        filepath=str(checkpoint_path),\n        global_step=global_step,\n        model_state_dict=model_state_dict,\n        optimizer_state_dict=optimizer_state_dict,\n        scheduler_state_dict=scheduler_state_dict,\n        kwargs=state_dict,\n    )\n</code></pre>"},{"location":"api/checkpoint/#llm.checkpoint.save_checkpoint","title":"save_checkpoint","text":"<pre><code>save_checkpoint(\n    checkpoint_dir: str | Path,\n    global_step: int,\n    model: Module,\n    optimizer: Optimizer | None = None,\n    scheduler: _LRScheduler | None = None,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p>Save checkpoint to directory.</p> <p>Saves the checkpoint as <code>{checkpoint_dir}/global_step_{global_step}.py</code>.</p> <p>Parameters:</p> <ul> <li> <code>checkpoint_dir</code>               (<code>str | Path</code>)           \u2013            <p>Directory to save checkpoint to.</p> </li> <li> <code>global_step</code>               (<code>int</code>)           \u2013            <p>Training step used as the key for checkpoints.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Model to save state_dict of.</p> </li> <li> <code>optimizer</code>               (<code>Optimizer | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional optimizer to save state_dict of.</p> </li> <li> <code>scheduler</code>               (<code>_LRScheduler | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional scheduler to save state_dict of.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional key-value pairs to add to the checkpoint.</p> </li> </ul> Source code in <code>llm/checkpoint.py</code> <pre><code>def save_checkpoint(\n    checkpoint_dir: str | pathlib.Path,\n    global_step: int,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer | None = None,\n    scheduler: torch.optim.lr_scheduler._LRScheduler | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Save checkpoint to directory.\n\n    Saves the checkpoint as `{checkpoint_dir}/global_step_{global_step}.py`.\n\n    Args:\n        checkpoint_dir: Directory to save checkpoint to.\n        global_step: Training step used as the key for checkpoints.\n        model: Model to save state_dict of.\n        optimizer: Optional optimizer to save state_dict of.\n        scheduler: Optional scheduler to save state_dict of.\n        kwargs: Additional key-value pairs to add to the checkpoint.\n    \"\"\"\n    state_dict = {'model': model.state_dict(), **kwargs}\n    if optimizer is not None:\n        state_dict['optimizer'] = optimizer.state_dict()\n    if scheduler is not None:\n        state_dict['scheduler'] = scheduler.state_dict()\n\n    dir_path = pathlib.Path(checkpoint_dir)\n    dir_path.mkdir(parents=True, exist_ok=True)\n    checkpoint_path = dir_path / f'global_step_{global_step}.pt'\n\n    torch.save(state_dict, checkpoint_path)\n</code></pre>"},{"location":"api/cli/","title":"CLI Reference","text":""},{"location":"api/cli/#cli-reference","title":"CLI Reference","text":"<p>This page provides documentation for our command line tools.</p> <p>Warning</p> <p>The usage examples show the executable module the CLI belongs to. To run the CLI, you must execute the module using the Python interpreter. E.g., <pre><code>$ python -m llm.preprocess.download --help\n</code></pre></p> <p>Note</p> <p>This list is not exhaustive. In particular, the training scripts provided in the <code>llm.trainers</code> module are not listed here.</p>"},{"location":"api/cli/#llmpreprocessdownload","title":"llm.preprocess.download","text":"<p>Pretraining text downloader.</p> <p>Usage:</p> <pre><code>llm.preprocess.download [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>-d</code>, <code>--dataset</code> choice (<code>wikipedia</code> | <code>bookscorpus</code>) Dataset to download. _required <code>-o</code>, <code>--output-dir</code> text Output directory. _required <code>--log-level</code> choice (<code>DEBUG</code> | <code>INFO</code> | <code>WARNING</code> | <code>ERROR</code>) Minimum logging level. <code>INFO</code> <code>--rich</code> / <code>--no-rich</code> boolean Use rich output formatting. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#llmpreprocessroberta","title":"llm.preprocess.roberta","text":"<p>Encode FILEPATHS for RoBERTa pretraining.</p> <p>Usage:</p> <pre><code>llm.preprocess.roberta [OPTIONS] FILEPATHS\n</code></pre> <p>Options:</p> Name Type Description Default <code>-o</code>, <code>--output-dir</code> text Output directory for encoded shards. _required <code>-t</code>, <code>--tokenizer</code> text Path to trained tokenizer to load. _required <code>-l</code>, <code>--max-seq-len</code> integer Maximum sequence length. <code>512</code> <code>-s</code>, <code>--short-seq-prob</code> float Probablity to create shorter sequences. <code>0.1</code> <code>-p</code>, <code>--processes</code> integer Number of processes for concurrent shard encoding. <code>4</code> <code>--log-level</code> choice (<code>DEBUG</code> | <code>INFO</code> | <code>WARNING</code> | <code>ERROR</code>) Minimum logging level. <code>INFO</code> <code>--rich</code> / <code>--no-rich</code> boolean Use rich output formatting. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#llmpreprocessshard","title":"llm.preprocess.shard","text":"<p>Shard documents in FILEPATHS into equally sized files.</p> <p>Usage:</p> <pre><code>llm.preprocess.shard [OPTIONS] FILEPATHS\n</code></pre> <p>Options:</p> Name Type Description Default <code>-o</code>, <code>--output-dir</code> text Output directory for encoded shards. _required <code>-s</code>, <code>--size</code> text Max data size of each shard. _required <code>-f</code>, <code>--format</code> text Shard name format where {index} is replaced by shard index. <code>shard-{index}.txt</code> <code>--shuffle</code> / <code>--no-shuffle</code> boolean Shuffle documents before sharding. <code>False</code> <code>--log-level</code> choice (<code>DEBUG</code> | <code>INFO</code> | <code>WARNING</code> | <code>ERROR</code>) Minimum logging level. <code>INFO</code> <code>--rich</code> / <code>--no-rich</code> boolean Use rich output formatting. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/cli/#llmpreprocesstokenizer","title":"llm.preprocess.tokenizer","text":"<p>Train a tokenizer on FILEPATHS.</p> <p>Arguments default to the standard uncased BERT with wordpiece method.</p> <p>Usage:</p> <pre><code>llm.preprocess.tokenizer [OPTIONS] FILEPATHS\n</code></pre> <p>Options:</p> Name Type Description Default <code>-o</code>, <code>--output-file</code> text Output file to save serialized tokenizer to. _required <code>-s</code>, <code>--size</code> integer Size of vocabulary. <code>30522</code> <code>-t</code>, <code>--tokenizer</code> choice (<code>bpe</code> | <code>wordpiece</code>) Tokenizer type. <code>wordpiece</code> <code>--cased</code> / <code>--uncased</code> boolean Vocab/tokenizer is case-sensitive. <code>False</code> <code>-s</code>, <code>--special-token</code> text Special tokens to prepend to vocab. <code>['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']</code> <code>--log-level</code> choice (<code>DEBUG</code> | <code>INFO</code> | <code>WARNING</code> | <code>ERROR</code>) Minimum logging level. <code>INFO</code> <code>--rich</code> / <code>--no-rich</code> boolean Use rich output formatting. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"api/config/","title":"llm.config","text":"<code>llm/config.py</code>"},{"location":"api/config/#llm.config.HParamT","title":"HParamT  <code>module-attribute</code>","text":"<pre><code>HParamT = Union[bool, float, int, str, None]\n</code></pre> <p>Supported Hyperparameter types (i.e., JSON types).</p>"},{"location":"api/config/#llm.config.Config","title":"Config","text":"<pre><code>Config(\n    mapping: (\n        Mapping[str, Any] | Iterable[tuple[str, Any]] | None\n    ) = None,\n    /,\n    **kwargs: Any,\n)\n</code></pre> <p>               Bases: <code>dict[str, Any]</code></p> <p>Dict-like configuration class with attribute access.</p> Example <pre><code>&gt;&gt;&gt; from llm.config import Config\n&gt;&gt;&gt; config = Config({'a': 1, 'b': 2})\n&gt;&gt;&gt; config.a\n1\n&gt;&gt;&gt; config = Config(a=1, b={'c': 2})\n&gt;&gt;&gt; config.b.c\n2\n</code></pre> <p>Parameters:</p> <ul> <li> <code>mapping</code>               (<code>Mapping[str, Any] | Iterable[tuple[str, Any]] | None</code>, default:                   <code>None</code> )           \u2013            <p>Initial mapping or iterable of tuples of key-value pairs.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keywords arguments to add a key-value pairs to the config.</p> </li> </ul> Source code in <code>llm/config.py</code> <pre><code>def __init__(\n    self,\n    mapping: Mapping[str, Any] | Iterable[tuple[str, Any]] | None = None,\n    /,\n    **kwargs: Any,\n):\n    if mapping is not None:\n        if isinstance(mapping, Mapping):\n            mapping = mapping.items()\n        for key, value in mapping:\n            self.__setattr__(key, value)\n\n    for key, value in kwargs.items():\n        self.__setattr__(key, value)\n</code></pre>"},{"location":"api/config/#llm.config.flattened_config","title":"flattened_config","text":"<pre><code>flattened_config(\n    config: dict[str, Any] | Config | None = None\n) -&gt; dict[str, HParamT]\n</code></pre> <p>Convert a config to a flat JSONable dictionary.</p> Note <p>If <code>torch.distributed.is_initialized()</code>, the <code>world_size</code> will be added to the config.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, Any] | Config | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional starting config.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, HParamT]</code>           \u2013            <p>Flat dictionary containing only <code>bool</code>, <code>float</code>, <code>int</code>, <code>str</code>, or</p> </li> <li> <code>dict[str, HParamT]</code>           \u2013            <p><code>None</code> values.</p> </li> </ul> Source code in <code>llm/config.py</code> <pre><code>def flattened_config(\n    config: dict[str, Any] | Config | None = None,\n) -&gt; dict[str, HParamT]:\n    \"\"\"Convert a config to a flat JSONable dictionary.\n\n    Note:\n        If\n        [`torch.distributed.is_initialized()`][torch.distributed.is_initialized],\n        the `world_size` will be added to the config.\n\n    Args:\n        config: Optional starting config.\n\n    Returns:\n        Flat dictionary containing only `bool`, `float`, `int`, `str`, or\n        `None` values.\n    \"\"\"\n    if config is None:\n        config = {}\n\n    if dist.is_initialized():\n        config['world_size'] = dist.get_world_size()\n\n    config = flatten_mapping(config)\n    for key in list(config.keys()):\n        if not isinstance(config[key], (bool, float, int, str, type(None))):\n            del config[key]\n\n    return config\n</code></pre>"},{"location":"api/config/#llm.config.flatten_mapping","title":"flatten_mapping","text":"<pre><code>flatten_mapping(\n    d: Mapping[str, Any],\n    parent: str | None = None,\n    sep: str = \"_\",\n) -&gt; dict[str, Any]\n</code></pre> <p>Flatten mapping into dict by joining nested keys via a separator.</p> Warning <p>This function does not check for key collisions. E.g., <pre><code>&gt;&gt;&gt; flatten_mapping({'a': {'b_c': 1}, 'a_b': {'c': 2}})\n{'a_b_c': 2}\n</code></pre></p> <p>Parameters:</p> <ul> <li> <code>d</code>               (<code>Mapping[str, Any]</code>)           \u2013            <p>Input mapping. All keys and nested keys must by strings.</p> </li> <li> <code>parent</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Parent key to prepend to top-level keys in <code>d</code>.</p> </li> <li> <code>sep</code>               (<code>str</code>, default:                   <code>'_'</code> )           \u2013            <p>Separator between keys.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>Flattened dictionary.</p> </li> </ul> Source code in <code>llm/config.py</code> <pre><code>def flatten_mapping(\n    d: Mapping[str, Any],\n    parent: str | None = None,\n    sep: str = '_',\n) -&gt; dict[str, Any]:\n    \"\"\"Flatten mapping into dict by joining nested keys via a separator.\n\n    Warning:\n        This function does not check for key collisions. E.g.,\n        ```python\n        &gt;&gt;&gt; flatten_mapping({'a': {'b_c': 1}, 'a_b': {'c': 2}})\n        {'a_b_c': 2}\n        ```\n\n    Args:\n        d: Input mapping. All keys and nested keys must by strings.\n        parent: Parent key to prepend to top-level keys in `d`.\n        sep: Separator between keys.\n\n    Returns:\n        Flattened dictionary.\n    \"\"\"\n    # https://stackoverflow.com/questions/6027558\n    items: list[tuple[str, Any]] = []\n\n    for key, value in d.items():\n        new_key = f'{parent}{sep}{key}' if parent is not None else key\n        if isinstance(value, collections.abc.Mapping):\n            items.extend(flatten_mapping(value, new_key, sep).items())\n        else:\n            items.append((new_key, value))\n\n    return dict(items)\n</code></pre>"},{"location":"api/config/#llm.config.load_config","title":"load_config","text":"<pre><code>load_config(filepath: Path | str) -&gt; Config\n</code></pre> <p>Load Python file as a <code>Config</code>.</p> Note <p>Attributes starting with <code>_</code>, modules, classes, functions, and builtins will not be loaded from the Python file.</p> <p>Parameters:</p> <ul> <li> <code>filepath</code>               (<code>Path | str</code>)           \u2013            <p>Python file to load.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Config</code>           \u2013            <p>Configuration attributes loaded from the Python file.</p> </li> </ul> Source code in <code>llm/config.py</code> <pre><code>def load_config(filepath: pathlib.Path | str) -&gt; Config:\n    \"\"\"Load Python file as a [`Config`][llm.config.Config].\n\n    Note:\n        Attributes starting with `_`, modules, classes, functions, and\n        builtins will not be loaded from the Python file.\n\n    Args:\n        filepath: Python file to load.\n\n    Returns:\n        Configuration attributes loaded from the Python file.\n    \"\"\"\n    filepath = pathlib.Path(filepath).absolute()\n    if not filepath.exists():\n        raise OSError(f'{filepath} does not exist.')\n    elif filepath.suffix != '.py':\n        raise ValueError(\n            f'{filepath} is not a Python file. Only .py files are supported.',\n        )\n\n    loader = SourceFileLoader(fullname=filepath.stem, path=str(filepath))\n    module = types.ModuleType(loader.name)\n    loader.exec_module(module)\n\n    attrs: dict[str, Any] = {}\n    for key, value in module.__dict__.items():\n        if not (\n            key.startswith('_')\n            or inspect.ismodule(value)\n            or inspect.isclass(value)\n            or inspect.isfunction(value)\n            or inspect.isbuiltin(value)\n        ):\n            attrs[key] = value\n\n    return Config(**attrs)\n</code></pre>"},{"location":"api/environment/","title":"llm.environment","text":"<code>llm/environment.py</code> <p>Utilities for collecting information about the environment.</p> Tip <p>This module is executable so you can easily check what resources your scripts will see as available. This is useful if you need to debug what software versions are being used or what hardware is visible by PyTorch. <pre><code>python -m llm.environment\n</code></pre></p>"},{"location":"api/environment/#llm.environment.Environment","title":"Environment","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Named tuple representing collected environment information.</p>"},{"location":"api/environment/#llm.environment.collect_pip_version","title":"collect_pip_version","text":"<pre><code>collect_pip_version() -&gt; str\n</code></pre> <p>Collect the pip version.</p> Source code in <code>llm/environment.py</code> <pre><code>def collect_pip_version() -&gt; str:\n    \"\"\"Collect the pip version.\"\"\"\n    output = subprocess.check_output(['pip', '--version']).decode('utf-8')\n    return output.split(' ')[1]\n</code></pre>"},{"location":"api/environment/#llm.environment.collect_pip_packages","title":"collect_pip_packages","text":"<pre><code>collect_pip_packages() -&gt; list[str]\n</code></pre> <p>Collect a list of relevant pip packages.</p> Source code in <code>llm/environment.py</code> <pre><code>def collect_pip_packages() -&gt; list[str]:\n    \"\"\"Collect a list of relevant pip packages.\"\"\"\n    output = subprocess.check_output(['pip', 'freeze']).decode('utf-8')\n    packages = output.split('\\n')\n    names = [\n        'torch',\n        'numpy',\n        'mypy',\n        'colossalai',\n        'h5py',\n        'tensorboard',\n        'tokenizers',\n        'transformers',\n    ]\n    packages = [\n        p.strip() for p in packages if any(name in p for name in names)\n    ]\n    return sorted(packages)\n</code></pre>"},{"location":"api/environment/#llm.environment.collect_environment","title":"collect_environment","text":"<pre><code>collect_environment() -&gt; Environment\n</code></pre> <p>Collects information on the hardware and software environment.</p> Source code in <code>llm/environment.py</code> <pre><code>def collect_environment() -&gt; Environment:\n    \"\"\"Collects information on the hardware and software environment.\"\"\"\n    run_lambda = collect_env.run\n\n    bit_count = sys.maxsize.bit_length() + 1\n    sys_version = sys.version.replace('\\n', ' ')\n\n    pip_version = collect_pip_version()\n    pip_packages = collect_pip_packages()\n    version_str = torch.__version__\n    debug_mode_str = torch.version.debug\n\n    pcores = psutil.cpu_count(logical=False)\n    lcores = psutil.cpu_count(logical=True)\n    cpu_info = f'{platform.processor()} ({pcores} cores / {lcores} logical)'\n    total_ram = round(psutil.virtual_memory().available / 1e9, 2)\n\n    cuda_available_str = torch.cuda.is_available()\n    cuda_version_str = torch.version.cuda\n\n    return Environment(\n        os=collect_env.get_os(run_lambda),\n        python_version=f'{sys_version} ({bit_count}-bit runtime)',\n        python_platform=collect_env.get_python_platform(),\n        pip_version=pip_version,\n        pip_packages='\\n'.join(pip_packages),\n        torch_version=version_str,\n        torch_is_debug=debug_mode_str,\n        cpu_info=cpu_info,\n        total_ram_gb=total_ram,\n        cuda_is_available=cuda_available_str,\n        cuda_compiled_version=cuda_version_str,\n        cuda_runtime_version=collect_env.get_running_cuda_version(run_lambda),\n        cuda_module_loading=collect_env.get_cuda_module_loading_config(),\n        nvidia_gpu_models=collect_env.get_gpu_info(run_lambda),\n        nvidia_driver=collect_env.get_nvidia_driver_version(run_lambda),\n        cudnn_version=collect_env.get_cudnn_version(run_lambda),\n    )\n</code></pre>"},{"location":"api/environment/#llm.environment.log_environment","title":"log_environment","text":"<pre><code>log_environment(\n    level: int = logging.INFO,\n    ranks: Iterable[int] | None = (0,),\n) -&gt; None\n</code></pre> <p>Log the hardware and software environment.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>int</code>, default:                   <code>INFO</code> )           \u2013            <p>Logging level.</p> </li> <li> <code>ranks</code>               (<code>Iterable[int] | None</code>, default:                   <code>(0,)</code> )           \u2013            <p>Ranks to log the environment on. If <code>None</code>, logs on all ranks.</p> </li> </ul> Source code in <code>llm/environment.py</code> <pre><code>def log_environment(\n    level: int = logging.INFO,\n    ranks: Iterable[int] | None = (0,),\n) -&gt; None:\n    \"\"\"Log the hardware and software environment.\n\n    Args:\n        level: Logging level.\n        ranks: Ranks to log the environment on. If `None`, logs on all ranks.\n    \"\"\"\n    env = collect_environment()\n    env_str = ENVIRONMENT_FORMAT.format(**env._asdict())\n    logger.log(\n        level,\n        f'Runtime environment:\\n{env_str}',\n        extra={'ranks': ranks},\n    )\n</code></pre>"},{"location":"api/initialize/","title":"llm.initialize","text":"<code>llm/initialize.py</code> <p>Utilities for initializing training environments.</p> <p>These utilities are used by the training scripts in <code>llm.trainers</code>.</p>"},{"location":"api/initialize/#llm.initialize.get_default_parser","title":"get_default_parser","text":"<pre><code>get_default_parser(\n    prog: str | None = None,\n    description: str | None = None,\n    usage: str | None = None,\n) -&gt; ArgumentParser\n</code></pre> <p>Get the default argument parser to be used with <code>initialize_from_args()</code>.</p> <p>Parameters:</p> <ul> <li> <code>prog</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional name of the program.</p> </li> <li> <code>description</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional description of the program.</p> </li> <li> <code>usage</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional program usage.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ArgumentParser</code>           \u2013            <p>Parser which you can append your own arguments to.</p> </li> </ul> Source code in <code>llm/initialize.py</code> <pre><code>def get_default_parser(\n    prog: str | None = None,\n    description: str | None = None,\n    usage: str | None = None,\n) -&gt; argparse.ArgumentParser:\n    \"\"\"Get the default argument parser to be used with [`initialize_from_args()`][llm.initialize.initialize_from_args].\n\n    Args:\n        prog: Optional name of the program.\n        description: Optional description of the program.\n        usage: Optional program usage.\n\n    Returns:\n        Parser which you can append your own arguments to.\n    \"\"\"  # noqa: E501\n    parser = argparse.ArgumentParser(\n        prog=prog,\n        description=description,\n        usage=usage,\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n    )\n    parser.add_argument('--config', help='path to config file')\n    parser.add_argument(\n        '--debug',\n        action='store_true',\n        help='single worker distributed configuration for debugging',\n    )\n    parser.add_argument(\n        '--loglevel',\n        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],\n        default='INFO',\n        help='minimum logging level',\n    )\n    parser.add_argument(\n        '--rich',\n        action='store_true',\n        help='use rich for pretty stdout logging',\n    )\n    return parser\n</code></pre>"},{"location":"api/initialize/#llm.initialize.initialize","title":"initialize","text":"<pre><code>initialize(\n    *,\n    debug: bool = False,\n    loglevel: int | str = \"INFO\",\n    logfile: Path | str | None = None,\n    seed: int | None = None,\n    rich: bool = False\n) -&gt; None\n</code></pre> <p>Initialize the distributed context.</p> <p>Perform the following: 1) initialize logging, 2) initialized torch distributed, and 3) set the cuda device is available.</p> <p>Parameters:</p> <ul> <li> <code>debug</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Initialize torch distributed for debugging (single worker).</p> </li> <li> <code>loglevel</code>               (<code>int | str</code>, default:                   <code>'INFO'</code> )           \u2013            <p>Minimum logging level.</p> </li> <li> <code>logfile</code>               (<code>Path | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Log filepath.</p> </li> <li> <code>seed</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Random seed.</p> </li> <li> <code>rich</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use rich formatting for stdout logging.</p> </li> </ul> Source code in <code>llm/initialize.py</code> <pre><code>def initialize(\n    *,\n    debug: bool = False,\n    loglevel: int | str = 'INFO',\n    logfile: pathlib.Path | str | None = None,\n    seed: int | None = None,\n    rich: bool = False,\n) -&gt; None:\n    \"\"\"Initialize the distributed context.\n\n    Perform the following: 1) initialize logging, 2) initialized torch\n    distributed, and 3) set the cuda device is available.\n\n    Args:\n        debug: Initialize torch distributed for debugging (single worker).\n        loglevel: Minimum logging level.\n        logfile: Log filepath.\n        seed: Random seed.\n        rich: Use rich formatting for stdout logging.\n    \"\"\"\n    init_logging(loglevel, logfile=logfile, rich=rich, distributed=True)\n\n    backend = 'nccl' if torch.cuda.is_available() else 'gloo'\n\n    if debug:\n        os.environ['LOCAL_RANK'] = '0'\n        os.environ['MASTER_ADDR'] = 'localhost'\n        os.environ['MASTER_PORT'] = '29501'\n        torch.distributed.init_process_group(\n            backend=backend,\n            world_size=1,\n            rank=0,\n        )\n    else:\n        torch.distributed.init_process_group(backend=backend)\n\n    logger.info(\n        'Distributed initialization complete: '\n        f'backend={torch.distributed.get_backend()}, '\n        f'world_size={torch.distributed.get_world_size()}',\n        extra={'ranks': [0]},\n    )\n\n    if torch.cuda.is_available():\n        local_rank = int(os.environ['LOCAL_RANK'])\n        torch.cuda.set_device(local_rank)\n\n        logger.info(\n            f'Initialized CUDA local device to {local_rank}',\n            extra={'ranks': [0]},\n        )\n\n    if seed is not None:\n        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n        random.seed(seed + local_rank)\n        numpy.random.seed(seed + local_rank)\n        torch.manual_seed(seed + local_rank)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed(seed + local_rank)\n</code></pre>"},{"location":"api/initialize/#llm.initialize.initialize_from_args","title":"initialize_from_args","text":"<pre><code>initialize_from_args(args: Namespace) -&gt; Config\n</code></pre> <p>Load config and initialize from args.</p> Example <pre><code>import sys\nfrom typing import Sequence\n\nfrom llm.initialize import get_default_parser\nfrom llm.initialize import initialize_from_args\n\ndef main(argv: Sequence[str] | None = None) -&gt; int:\n    argv = argv if argv is not None else sys.argv[1:]\n    parser = get_default_parser()\n    args = parser.parse_args(argv)\n    config = initialize_from_args(args)\n\n    # Rest of your training script\n\nif __name__ == '__main__':\n    raise SystemExit(main())\n</code></pre> Source code in <code>llm/initialize.py</code> <pre><code>def initialize_from_args(args: argparse.Namespace) -&gt; Config:\n    \"\"\"Load config and initialize from args.\n\n    Example:\n        ```python\n        import sys\n        from typing import Sequence\n\n        from llm.initialize import get_default_parser\n        from llm.initialize import initialize_from_args\n\n        def main(argv: Sequence[str] | None = None) -&gt; int:\n            argv = argv if argv is not None else sys.argv[1:]\n            parser = get_default_parser()\n            args = parser.parse_args(argv)\n            config = initialize_from_args(args)\n\n            # Rest of your training script\n\n        if __name__ == '__main__':\n            raise SystemExit(main())\n        ```\n    \"\"\"\n    config = load_config(args.config)\n\n    initialize(\n        debug=args.debug,\n        loglevel=args.loglevel or config.get('LOG_LEVEL', None),\n        logfile=config.get('LOG_FILE', None),\n        seed=config.get('SEED', None),\n        rich=args.rich,\n    )\n\n    return config\n</code></pre>"},{"location":"api/kfac/","title":"llm.kfac","text":"<code>llm/kfac.py</code>"},{"location":"api/kfac/#llm.kfac.add_kfac_options","title":"add_kfac_options","text":"<pre><code>add_kfac_options(parser: ArgumentParser) -&gt; None\n</code></pre> <p>Add CLI arguments for K-FAC.</p> <p>Parameters:</p> <ul> <li> <code>parser</code>               (<code>ArgumentParser</code>)           \u2013            <p>Parser object to add K-FAC arguments to.</p> </li> </ul> Source code in <code>llm/kfac.py</code> <pre><code>def add_kfac_options(parser: argparse.ArgumentParser) -&gt; None:\n    \"\"\"Add CLI arguments for K-FAC.\n\n    Args:\n        parser: Parser object to add K-FAC arguments to.\n    \"\"\"\n    args_str = ' '.join(sys.argv)\n    group = parser.add_argument_group(\n        title='K-FAC',\n        description='K-FAC preconditioning options',\n    )\n    group.add_argument(\n        '--kfac',\n        action='store_true',\n        help='Use K-FAC for preconditioning',\n    )\n    group.add_argument(\n        '--kfac-factor-update-steps',\n        type=int,\n        required=bool(re.search(r'--kfac($|\\s)', args_str)),\n        help='Steps between updating Kronecker factors',\n    )\n    group.add_argument(\n        '--kfac-inv-update-steps',\n        type=int,\n        required=bool(re.search(r'--kfac($|\\s)', args_str)),\n        help='Steps between recomputing inverses/eigen decompositions',\n    )\n    group.add_argument(\n        '--kfac-damping',\n        type=float,\n        default=0.001,\n        help='K-FAC damping value',\n    )\n    group.add_argument(\n        '--kfac-factor-decay',\n        type=float,\n        default=0.95,\n        help='K-FAC factor decay rate',\n    )\n    group.add_argument(\n        '--kfac-kl-clip',\n        type=float,\n        default=0.001,\n        help='K-FAC KL-clip',\n    )\n</code></pre>"},{"location":"api/loss/","title":"llm.loss","text":"<code>llm/loss.py</code> <p>Training loss functions.</p>"},{"location":"api/loss/#llm.loss.BertPretrainingCriterion","title":"BertPretrainingCriterion","text":"<pre><code>BertPretrainingCriterion(\n    vocab_size: int, ignore_index: int = -100\n)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>BERT pretraining loss.</p> <p>Computes the sum of the cross entropy losses of the masked language model and (optionally) next sentence prediction tasks.</p> <p>Parameters:</p> <ul> <li> <code>vocab_size</code>               (<code>int</code>)           \u2013            <p>Size of the pretraining vocabulary.</p> </li> <li> <code>ignore_index</code>               (<code>int</code>, default:                   <code>-100</code> )           \u2013            <p>Value to ignore when computing cross entropy loss. Defaults to -100 which is used by the provided BERT datasets as the value in <code>masked_lm_labels</code> which are not masked.</p> </li> </ul> Source code in <code>llm/loss.py</code> <pre><code>def __init__(self, vocab_size: int, ignore_index: int = -100) -&gt; None:\n    super().__init__()\n    self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=ignore_index)\n    self.vocab_size = vocab_size\n</code></pre>"},{"location":"api/loss/#llm.loss.BertPretrainingCriterion.forward","title":"forward","text":"<pre><code>forward(\n    prediction_scores: FloatTensor,\n    masked_lm_labels: LongTensor,\n    seq_relationship_score: FloatTensor | None = None,\n    next_sentence_labels: LongTensor | None = None,\n) -&gt; float\n</code></pre> <p>Compute the pretraining loss.</p> <p>Parameters:</p> <ul> <li> <code>prediction_scores</code>               (<code>FloatTensor</code>)           \u2013            <p>Masked token predictions.</p> </li> <li> <code>masked_lm_labels</code>               (<code>LongTensor</code>)           \u2013            <p>True masked token labels.</p> </li> <li> <code>seq_relationship_score</code>               (<code>FloatTensor | None</code>, default:                   <code>None</code> )           \u2013            <p>Predicted sequence relationship score.</p> </li> <li> <code>next_sentence_labels</code>               (<code>LongTensor | None</code>, default:                   <code>None</code> )           \u2013            <p>True next sentence label.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float</code>           \u2013            <p>Computed loss.</p> </li> </ul> Source code in <code>llm/loss.py</code> <pre><code>def forward(\n    self,\n    prediction_scores: torch.FloatTensor,\n    masked_lm_labels: torch.LongTensor,\n    seq_relationship_score: torch.FloatTensor | None = None,\n    next_sentence_labels: torch.LongTensor | None = None,\n) -&gt; float:\n    \"\"\"Compute the pretraining loss.\n\n    Args:\n        prediction_scores: Masked token predictions.\n        masked_lm_labels: True masked token labels.\n        seq_relationship_score: Predicted sequence relationship score.\n        next_sentence_labels: True next sentence label.\n\n    Returns:\n        Computed loss.\n    \"\"\"\n    masked_lm_loss = self.loss_fn(\n        prediction_scores.view(-1, self.vocab_size),\n        masked_lm_labels.view(-1),\n    )\n\n    if (\n        seq_relationship_score is not None\n        and next_sentence_labels is not None\n    ):\n        next_sentence_loss = self.loss_fn(\n            seq_relationship_score.view(-1, 2),\n            next_sentence_labels.view(-1),\n        )\n        masked_lm_loss += next_sentence_loss\n\n    return masked_lm_loss\n</code></pre>"},{"location":"api/optimizers/","title":"llm.optimizers","text":"<code>llm/optimizers.py</code> <p>Training optimizers.</p>"},{"location":"api/optimizers/#llm.optimizers.get_optimizer","title":"get_optimizer","text":"<pre><code>get_optimizer(\n    name: Literal[\"lamb\", \"adam\"],\n    params: Iterable[Tensor] | Iterable[dict[str, Any]],\n    lr: float,\n    **kwargs: Any\n) -&gt; Optimizer\n</code></pre> <p>Get an optimizer by name.</p> Note <p>If ColossalAI is installed, fused versions of the optimizers will be created instead.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Literal['lamb', 'adam']</code>)           \u2013            <p>Name of the optimizer to load.</p> </li> <li> <code>params</code>               (<code>Iterable[Tensor] | Iterable[dict[str, Any]]</code>)           \u2013            <p>Parameters to be optimized.</p> </li> <li> <code>lr</code>               (<code>float</code>)           \u2013            <p>Learning rate.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to the optimizer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Optimizer</code>           \u2013            <p>Initialized optimizer.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ImportError</code>             \u2013            <p>If <code>name=='lamb'</code> and ColossalAI is not installed.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If <code>name</code> is unknown.</p> </li> </ul> Source code in <code>llm/optimizers.py</code> <pre><code>def get_optimizer(\n    name: Literal['lamb', 'adam'],\n    params: Iterable[torch.Tensor] | Iterable[dict[str, Any]],\n    lr: float,\n    **kwargs: Any,\n) -&gt; torch.optim.Optimizer:\n    \"\"\"Get an optimizer by name.\n\n    Note:\n        If [ColossalAI](https://github.com/hpcaitech/ColossalAI){target=_blank}\n        is installed, fused versions of the optimizers will be created instead.\n\n    Args:\n        name: Name of the optimizer to load.\n        params: Parameters to be optimized.\n        lr: Learning rate.\n        kwargs: Keyword arguments to pass to the optimizer.\n\n    Returns:\n        Initialized optimizer.\n\n    Raises:\n        ImportError: If `name=='lamb'` and ColossalAI is not installed.\n        ValueError: If `name` is unknown.\n    \"\"\"\n    if name == 'adam':  # pragma: no cover\n        if FUSED_IMPORT_ERROR is None:\n            optimizer = FusedAdam(params, lr=lr, **kwargs)\n        else:\n            logger.warning(\n                'ColossalAI with CUDA extensions is not installed so '\n                'defaulting to native PyTorch Adam. Better performance can be '\n                \"enabled with ColossalAI's FusedAdam.\",\n            )\n            optimizer = torch.optim.Adam(params, lr=lr, **kwargs)\n    elif name == 'lamb':  # pragma: no cover\n        if FUSED_IMPORT_ERROR is None:\n            optimizer = FusedLAMB(params, lr=lr, **kwargs)\n        else:\n            raise ImportError(\n                'FusedLamb is not available. ColossalAI with CUDA extensions '\n                'is not installed.',\n            ) from FUSED_IMPORT_ERROR\n    else:\n        raise ValueError(f'Unknown optimizer: {name}')\n\n    return optimizer\n</code></pre>"},{"location":"api/schedulers/","title":"llm.schedulers","text":"<code>llm/schedulers.py</code> <p>Custom learning rate schedules.</p>"},{"location":"api/schedulers/#llm.schedulers.LinearWarmupLR","title":"LinearWarmupLR","text":"<pre><code>LinearWarmupLR(\n    optimizer: Optimizer,\n    total_steps: int,\n    warmup_steps: int = 0,\n    last_epoch: int = -1,\n)\n</code></pre> <p>               Bases: <code>_LRScheduler</code></p> <p>Linear warmup and decay LR scheduler.</p> <p>Source: ColossalAI</p> <p>Parameters:</p> <ul> <li> <code>optimizer</code>               (<code>Optimizer</code>)           \u2013            <p>Optimizer to adjust learning rate of.</p> </li> <li> <code>total_steps</code>               (<code>int</code>)           \u2013            <p>Total training steps.</p> </li> <li> <code>warmup_steps</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Steps to linearly warmup the learning rate.</p> </li> <li> <code>last_epoch</code>               (<code>int</code>, default:                   <code>-1</code> )           \u2013            <p>Optional last epoch.</p> </li> </ul> Source code in <code>llm/schedulers.py</code> <pre><code>def __init__(\n    self,\n    optimizer: torch.optim.Optimizer,\n    total_steps: int,\n    warmup_steps: int = 0,\n    last_epoch: int = -1,\n) -&gt; None:\n    self.total_steps = total_steps\n    self.warmup_steps = warmup_steps\n    super().__init__(optimizer, last_epoch)\n</code></pre>"},{"location":"api/schedulers/#llm.schedulers.LinearWarmupLR.get_lr","title":"get_lr","text":"<pre><code>get_lr() -&gt; list[float]\n</code></pre> <p>Compute the current learning rate.</p> Source code in <code>llm/schedulers.py</code> <pre><code>def get_lr(self) -&gt; list[float]:\n    \"\"\"Compute the current learning rate.\"\"\"\n    if self.last_epoch &lt; self.warmup_steps:\n        factor = (self.last_epoch + 1) / (self.warmup_steps + 1)\n        return [base_lr * factor for base_lr in self.base_lrs]\n    else:\n        factor = self.total_steps - self.last_epoch\n        factor = factor / (self.total_steps - self.warmup_steps)\n        return [base_lr * factor for base_lr in self.base_lrs]\n</code></pre>"},{"location":"api/timer/","title":"llm.timer","text":"<code>llm/timer.py</code> <p>Performance timer.</p>"},{"location":"api/timer/#llm.timer.Timer","title":"Timer","text":"<pre><code>Timer(synchronize: bool = False)\n</code></pre> <p>Performance timer.</p> <p>Parameters:</p> <ul> <li> <code>synchronize</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Synchronize CUDA workers before and after the timer.</p> </li> </ul> Source code in <code>llm/timer.py</code> <pre><code>def __init__(self, synchronize: bool = False) -&gt; None:\n    self._synchronize = synchronize\n    self._history: list[float] = []\n    self._start_time = time.time()\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.start","title":"start","text":"<pre><code>start() -&gt; None\n</code></pre> <p>Start the timer.</p> Source code in <code>llm/timer.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the timer.\"\"\"\n    if torch.cuda.is_available() and self._synchronize:\n        torch.cuda.synchronize()\n    self._start_time = time.time()\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.stop","title":"stop","text":"<pre><code>stop() -&gt; float\n</code></pre> <p>Stop the timer and return the elapsed time.</p> Source code in <code>llm/timer.py</code> <pre><code>def stop(self) -&gt; float:\n    \"\"\"Stop the timer and return the elapsed time.\"\"\"\n    if torch.cuda.is_available() and self._synchronize:\n        torch.cuda.synchronize()\n\n    elapsed = time.time() - self._start_time\n    self._history.append(elapsed)\n    return elapsed\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.lap","title":"lap","text":"<pre><code>lap() -&gt; float\n</code></pre> <p>Log a lap of the timer.</p> Source code in <code>llm/timer.py</code> <pre><code>def lap(self) -&gt; float:\n    \"\"\"Log a lap of the timer.\"\"\"\n    lap_time = self.stop()\n    self.start()\n    return lap_time\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.get_history","title":"get_history","text":"<pre><code>get_history() -&gt; list[float]\n</code></pre> <p>Get the history of all lap times.</p> Source code in <code>llm/timer.py</code> <pre><code>def get_history(self) -&gt; list[float]:\n    \"\"\"Get the history of all lap times.\"\"\"\n    return self._history\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.get_history_sum","title":"get_history_sum","text":"<pre><code>get_history_sum() -&gt; float\n</code></pre> <p>Get the sum of all lap times.</p> Source code in <code>llm/timer.py</code> <pre><code>def get_history_sum(self) -&gt; float:\n    \"\"\"Get the sum of all lap times.\"\"\"\n    return sum(self._history)\n</code></pre>"},{"location":"api/timer/#llm.timer.Timer.get_history_mean","title":"get_history_mean","text":"<pre><code>get_history_mean() -&gt; float\n</code></pre> <p>Get the mean of all lap times.</p> Source code in <code>llm/timer.py</code> <pre><code>def get_history_mean(self) -&gt; float:\n    \"\"\"Get the mean of all lap times.\"\"\"\n    return sum(self._history) / len(self._history)\n</code></pre>"},{"location":"api/utils/","title":"llm.utils","text":"<code>llm/utils.py</code>"},{"location":"api/utils/#llm.utils.HParamT","title":"HParamT  <code>module-attribute</code>","text":"<pre><code>HParamT = Union[bool, float, int, str, None]\n</code></pre> <p>Supported Hyperparameter types (i.e., JSON types).</p>"},{"location":"api/utils/#llm.utils.DistributedFilter","title":"DistributedFilter","text":"<p>               Bases: <code>Filter</code></p> <p>Custom filter that allows specifying ranks to print to.</p> Example <pre><code>logger.info('My log message', extra={'ranks': [0, 2]})\n</code></pre>"},{"location":"api/utils/#llm.utils.create_summary_writer","title":"create_summary_writer","text":"<pre><code>create_summary_writer(\n    tensorboard_dir: str,\n    hparam_dict: dict[str, HParamT] | None = None,\n    metrics: list[str] | None = None,\n    **writer_kwargs: Any\n) -&gt; SummaryWriter\n</code></pre> <p>Create a SummaryWriter instance for the run annotated with hyperparams.</p> <p>https://github.com/pytorch/pytorch/issues/37738#issuecomment-1124497827</p> <p>Parameters:</p> <ul> <li> <code>tensorboard_dir</code>               (<code>str</code>)           \u2013            <p>TensorBoard run directory.</p> </li> <li> <code>hparam_dict</code>               (<code>dict[str, HParamT] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional hyperparam dictionary to log alongside metrics.</p> </li> <li> <code>metrics</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional list of metric tags that will be used with <code>writer.add_scalar()</code> (e.g., <code>['train/loss', 'train/lr']</code>). Must be provided if <code>hparam_dict</code> is provided.</p> </li> <li> <code>writer_kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to pass to <code>SummaryWriter</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>SummaryWriter</code>           \u2013            <p>Summary writer instance.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def create_summary_writer(\n    tensorboard_dir: str,\n    hparam_dict: dict[str, HParamT] | None = None,\n    metrics: list[str] | None = None,\n    **writer_kwargs: Any,\n) -&gt; SummaryWriter:\n    \"\"\"Create a SummaryWriter instance for the run annotated with hyperparams.\n\n    https://github.com/pytorch/pytorch/issues/37738#issuecomment-1124497827\n\n    Args:\n        tensorboard_dir: TensorBoard run directory.\n        hparam_dict: Optional hyperparam dictionary to log alongside\n            metrics.\n        metrics: Optional list of metric tags that will be used with\n            [`writer.add_scalar()`][torch.utils.tensorboard.writer.SummaryWriter.add_scalar]\n            (e.g., `['train/loss', 'train/lr']`). Must be provided if\n            `hparam_dict` is provided.\n        writer_kwargs: Additional keyword arguments to pass to\n            `SummaryWriter`.\n\n    Returns:\n        Summary writer instance.\n    \"\"\"\n    writer = SummaryWriter(tensorboard_dir, **writer_kwargs)\n\n    if hparam_dict is not None and metrics is not None:\n        metric_dict = dict.fromkeys(metrics, 0)\n        exp, ssi, sei = hparams(hparam_dict, metric_dict=metric_dict)\n        assert writer.file_writer is not None\n        writer.file_writer.add_summary(exp)\n        writer.file_writer.add_summary(ssi)\n        writer.file_writer.add_summary(sei)\n\n    return writer\n</code></pre>"},{"location":"api/utils/#llm.utils.get_filepaths","title":"get_filepaths","text":"<pre><code>get_filepaths(\n    directory: Path | str,\n    extensions: list[str] | None = None,\n    recursive: bool = False,\n) -&gt; list[str]\n</code></pre> <p>Get list of filepaths in directory.</p> Note <p>Only files (not sub-directories will be returned. Though sub-directories will be recursed into if <code>recursive=True</code>.</p> <p>Parameters:</p> <ul> <li> <code>directory</code>               (<code>Path | str</code>)           \u2013            <p>Pathlike object with the directory to search.</p> </li> <li> <code>extensions</code>               (<code>list[str] | None</code>, default:                   <code>None</code> )           \u2013            <p>Pptionally only return files that match these extensions. Each extension should include the dot. E.g., <code>['.pdf', '.txt']</code>. Match is case sensitive.</p> </li> <li> <code>recursive</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Recursively search sub-directories.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[str]</code>           \u2013            <p>List of string paths of files in the directory.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def get_filepaths(\n    directory: pathlib.Path | str,\n    extensions: list[str] | None = None,\n    recursive: bool = False,\n) -&gt; list[str]:\n    \"\"\"Get list of filepaths in directory.\n\n    Note:\n        Only files (not sub-directories will be returned. Though\n        sub-directories will be recursed into if `recursive=True`.\n\n    Args:\n        directory: Pathlike object with the directory to search.\n        extensions: Pptionally only return files that match these extensions.\n            Each extension should include the dot. E.g., `['.pdf', '.txt']`.\n            Match is case sensitive.\n        recursive: Recursively search sub-directories.\n\n    Returns:\n        List of string paths of files in the directory.\n    \"\"\"\n    directory = pathlib.Path(directory)\n    glob = '**/*' if recursive else '*'\n\n    files = [path for path in directory.glob(glob) if path.is_file()]\n    if extensions is not None:\n        files = [path for path in files if path.suffix in extensions]\n    return [str(path) for path in files]\n</code></pre>"},{"location":"api/utils/#llm.utils.gradient_accumulation_steps","title":"gradient_accumulation_steps","text":"<pre><code>gradient_accumulation_steps(\n    global_batch_size: int,\n    local_batch_size: int,\n    world_size: int,\n) -&gt; int\n</code></pre> <p>Compute the gradient accumulation steps from the configuration.</p> <p>Parameters:</p> <ul> <li> <code>global_batch_size</code>               (<code>int</code>)           \u2013            <p>Target global/effective batch size.</p> </li> <li> <code>local_batch_size</code>               (<code>int</code>)           \u2013            <p>Per rank batch size.</p> </li> <li> <code>world_size</code>               (<code>int</code>)           \u2013            <p>Number of ranks.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Gradient accumulation steps needed to achieve the <code>global_batch_size</code>.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the resulting gradient accumulation steps would be fractional.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def gradient_accumulation_steps(\n    global_batch_size: int,\n    local_batch_size: int,\n    world_size: int,\n) -&gt; int:\n    \"\"\"Compute the gradient accumulation steps from the configuration.\n\n    Args:\n        global_batch_size: Target global/effective batch size.\n        local_batch_size: Per rank batch size.\n        world_size: Number of ranks.\n\n    Returns:\n        Gradient accumulation steps needed to achieve the `global_batch_size`.\n\n    Raises:\n        ValueError: If the resulting gradient accumulation steps would be\n            fractional.\n    \"\"\"\n    effective_batch = local_batch_size * world_size\n\n    if global_batch_size % effective_batch != 0:\n        raise ValueError(\n            f'The global batch size ({global_batch_size}) must be evenly '\n            'divisible by the product of the local batch size '\n            f'({local_batch_size}) and the world size ({world_size}).',\n        )\n\n    return global_batch_size // effective_batch\n</code></pre>"},{"location":"api/utils/#llm.utils.init_logging","title":"init_logging","text":"<pre><code>init_logging(\n    level: int | str = logging.INFO,\n    logfile: Path | str | None = None,\n    rich: bool = False,\n    distributed: bool = False,\n) -&gt; None\n</code></pre> <p>Configure global logging.</p> <p>Parameters:</p> <ul> <li> <code>level</code>               (<code>int | str</code>, default:                   <code>INFO</code> )           \u2013            <p>Default logging level.</p> </li> <li> <code>logfile</code>               (<code>Path | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional path to write logs to.</p> </li> <li> <code>rich</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Use rich for pretty stdout logging.</p> </li> <li> <code>distributed</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Configure distributed formatters and filters.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def init_logging(\n    level: int | str = logging.INFO,\n    logfile: pathlib.Path | str | None = None,\n    rich: bool = False,\n    distributed: bool = False,\n) -&gt; None:\n    \"\"\"Configure global logging.\n\n    Args:\n        level: Default logging level.\n        logfile: Optional path to write logs to.\n        rich: Use rich for pretty stdout logging.\n        distributed: Configure distributed formatters and filters.\n    \"\"\"\n    formatter = logging.Formatter(\n        fmt='[%(asctime)s.%(msecs)03d] %(levelname)s (%(name)s): %(message)s',\n        datefmt='%Y-%m-%d %H:%M:%S',\n    )\n\n    stdout_handler: logging.Handler\n    if rich:\n        stdout_handler = RichHandler(rich_tracebacks=True)\n    else:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.setFormatter(formatter)\n    handlers: list[logging.Handler] = [stdout_handler]\n\n    if logfile is not None:\n        path = pathlib.Path(logfile).resolve()\n        path.parent.mkdir(parents=True, exist_ok=True)\n        file_handler = logging.FileHandler(path)\n        file_handler.setFormatter(formatter)\n        handlers.append(file_handler)\n\n    if distributed:\n        filter_ = DistributedFilter()\n        for handler in handlers:\n            handler.addFilter(filter_)\n\n    logging.basicConfig(\n        level=level,\n        format='%(message)s',\n        datefmt='[%X]',\n        handlers=handlers,\n    )\n</code></pre>"},{"location":"api/utils/#llm.utils.log_step","title":"log_step","text":"<pre><code>log_step(\n    logger: Logger,\n    step: int,\n    *,\n    fmt_str: str | None = None,\n    log_level: int = logging.INFO,\n    ranks: Iterable[int] = (0,),\n    skip_tensorboard: Iterable[str] = (),\n    tensorboard_prefix: str = \"train\",\n    writer: SummaryWriter | None = None,\n    **kwargs: Any\n) -&gt; None\n</code></pre> <p>Log a training step.</p> <p>Parameters:</p> <ul> <li> <code>logger</code>               (<code>Logger</code>)           \u2013            <p>Logger instance to log to.</p> </li> <li> <code>step</code>               (<code>int</code>)           \u2013            <p>Training step.</p> </li> <li> <code>fmt_str</code>               (<code>str | None</code>, default:                   <code>None</code> )           \u2013            <p>Format string used to format parameters for logging.</p> </li> <li> <code>log_level</code>               (<code>int</code>, default:                   <code>INFO</code> )           \u2013            <p>Level to log the parameters at.</p> </li> <li> <code>ranks</code>               (<code>Iterable[int]</code>, default:                   <code>(0,)</code> )           \u2013            <p>Ranks to log on (default to rank 0 only).</p> </li> <li> <code>skip_tensorboard</code>               (<code>Iterable[str]</code>, default:                   <code>()</code> )           \u2013            <p>List of parameter names to skip logging to TensorBoard.</p> </li> <li> <code>tensorboard_prefix</code>               (<code>str</code>, default:                   <code>'train'</code> )           \u2013            <p>Prefix for TensorBoard parameters.</p> </li> <li> <code>writer</code>               (<code>SummaryWriter | None</code>, default:                   <code>None</code> )           \u2013            <p>TensorBoard summary writer.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to log.</p> </li> </ul> Source code in <code>llm/utils.py</code> <pre><code>def log_step(\n    logger: logging.Logger,\n    step: int,\n    *,\n    fmt_str: str | None = None,\n    log_level: int = logging.INFO,\n    ranks: Iterable[int] = (0,),\n    skip_tensorboard: Iterable[str] = (),\n    tensorboard_prefix: str = 'train',\n    writer: SummaryWriter | None = None,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"Log a training step.\n\n    Args:\n        logger: Logger instance to log to.\n        step: Training step.\n        fmt_str: Format string used to format parameters for logging.\n        log_level: Level to log the parameters at.\n        ranks: Ranks to log on (default to rank 0 only).\n        skip_tensorboard: List of parameter names to skip logging to\n            TensorBoard.\n        tensorboard_prefix: Prefix for TensorBoard parameters.\n        writer: TensorBoard summary writer.\n        kwargs: Additional keyword arguments to log.\n    \"\"\"\n    values = {'step': step, **kwargs}\n    if fmt_str is not None:\n        msg = fmt_str.format(**values)\n    else:\n        msg = ' | '.join(f'{name}: {value}' for name, value in values.items())\n\n    logger.log(log_level, msg, extra={'ranks': ranks})\n    if writer is not None:\n        for name, value in kwargs.items():\n            if name not in skip_tensorboard:\n                writer.add_scalar(f'{tensorboard_prefix}/{name}', value, step)\n</code></pre>"},{"location":"api/datasets/","title":"llm.datasets","text":"<code>llm/datasets/__init__.py</code>"},{"location":"api/datasets/bert/","title":"llm.datasets.bert","text":"<code>llm/datasets/bert.py</code> <p>NVIDIA BERT dataset provider.</p> <p>Source: https://github.com/hpcaitech/ColossalAI-Examples/blob/e0830ccc1bbc57f9c50bb1c00f3e23239bf1e231/language/roberta/pretraining/nvidia_bert_dataset_provider.py</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch","title":"Batch","text":"<p>               Bases: <code>NamedTuple</code></p> <p>BERT pretraining batch.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.input_ids","title":"input_ids  <code>instance-attribute</code>","text":"<pre><code>input_ids: LongTensor\n</code></pre> <p>Input sequence token IDs (<code>(batch_size, seq_len)</code>).</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.attention_mask","title":"attention_mask  <code>instance-attribute</code>","text":"<pre><code>attention_mask: LongTensor\n</code></pre> <p>Input sequence attention mask (<code>(batch_size, seq_len)</code>).</p> <p>Indicates which tokens in <code>input_ids</code> should be attended to (i.e., are not padding tokens). Also known as the input mask.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.token_type_ids","title":"token_type_ids  <code>instance-attribute</code>","text":"<pre><code>token_type_ids: LongTensor\n</code></pre> <p>Token IDs indicating the segment labels (<code>(batch_size, seq_len)</code>).</p> <p>E.g. if <code>input_ids</code> is composed of two distinct segments, the first segment will have token IDs set to 0 and the second to 1. Also known as the segment ids.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.masked_labels","title":"masked_labels  <code>instance-attribute</code>","text":"<pre><code>masked_labels: LongTensor\n</code></pre> <p>True token ID of masked tokens in <code>input_ids</code> (<code>(batch_size, seq_len)</code>).</p> <p>Indices corresponding to non-masked tokens in <code>input_ids</code> are typically set to <code>-100</code> to avoid contributing to the MLM loss.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Batch.next_sentence_labels","title":"next_sentence_labels  <code>instance-attribute</code>","text":"<pre><code>next_sentence_labels: LongTensor | None\n</code></pre> <p>Boolean tensor indicating the next sentence label (<code>(batch_size,)</code>).</p> <p>A true (<code>1</code>) value indicates the next sentence/segment logically follows the first. If there is only one segment in <code>input_ids</code>, this can be set to <code>None</code>.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample","title":"Sample","text":"<p>               Bases: <code>NamedTuple</code></p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.input_ids","title":"input_ids  <code>instance-attribute</code>","text":"<pre><code>input_ids: LongTensor\n</code></pre> <p>Input sequence token IDs (<code>(seq_len,)</code>).</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.attention_mask","title":"attention_mask  <code>instance-attribute</code>","text":"<pre><code>attention_mask: LongTensor\n</code></pre> <p>Input sequence attention mask (<code>(seq_len,)</code>).</p> <p>Indicates which tokens in <code>input_ids</code> should be attended to (i.e., are not padding tokens). Also known as the input mask.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.token_type_ids","title":"token_type_ids  <code>instance-attribute</code>","text":"<pre><code>token_type_ids: LongTensor\n</code></pre> <p>Token IDs indicating the segment labels (<code>(seq_len,)</code>).</p> <p>E.g. if <code>input_ids</code> is composed of two distinct segments, the first segment will have token IDs set to 0 and the second to 1. Also known as the segment ids.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.masked_labels","title":"masked_labels  <code>instance-attribute</code>","text":"<pre><code>masked_labels: LongTensor\n</code></pre> <p>True token ID of masked tokens in <code>input_ids</code> (<code>(seq_len,)</code>).</p> <p>Indices corresponding to non-masked tokens in <code>input_ids</code> are typically set to <code>-100</code> to avoid contributing to the MLM loss.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.Sample.next_sentence_label","title":"next_sentence_label  <code>instance-attribute</code>","text":"<pre><code>next_sentence_label: LongTensor | None\n</code></pre> <p>Boolean scalar tensor indicating the next sentence label.</p> <p>A true (<code>1</code>) value indicates the next sentence/segment logically follows the first. If there is only one segment in <code>input_ids</code>, this can be set to <code>None</code>.</p>"},{"location":"api/datasets/bert/#llm.datasets.bert.NvidiaBertDataset","title":"NvidiaBertDataset","text":"<pre><code>NvidiaBertDataset(input_file: str)\n</code></pre> <p>               Bases: <code>Dataset[Sample]</code></p> <p>NVIDIA BERT dataset.</p> <p>Like the PyTorch <code>Dataset</code>, this dataset is indexable returning a <code>Sample</code>.</p> Example <pre><code>&gt;&gt;&gt; from llm.datasets.bert import NvidiaBertDataset\n&gt;&gt;&gt; dataset = NvidiaBertDataset('/path/to/shard')\n&gt;&gt;&gt; dataset[5]\nSample(...)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>input_file</code>               (<code>str</code>)           \u2013            <p>HDF5 file to load.</p> </li> </ul> Source code in <code>llm/datasets/bert.py</code> <pre><code>def __init__(self, input_file: str) -&gt; None:\n    self.input_file = input_file\n\n    # We just inspect the file enough to find the size of the dataset\n    # then defer loading until we actually use the dataset. This is\n    # particularly helpful for the DistributedShardedDataset which needs\n    # to get the length of each shard\n    self.loaded = False\n    with h5py.File(self.input_file, 'r') as f:\n        self.samples = len(f['next_sentence_labels'])\n</code></pre>"},{"location":"api/datasets/bert/#llm.datasets.bert.get_masked_labels","title":"get_masked_labels","text":"<pre><code>get_masked_labels(\n    seq_len: int,\n    masked_lm_positions: list[int],\n    masked_lm_ids: list[int],\n) -&gt; ndarray[Any, Any]\n</code></pre> <p>Create masked labels array.</p> <p>Parameters:</p> <ul> <li> <code>seq_len</code>               (<code>int</code>)           \u2013            <p>Sequence length.</p> </li> <li> <code>masked_lm_positions</code>               (<code>list[int]</code>)           \u2013            <p>Index in sequence of masked tokens</p> </li> <li> <code>masked_lm_ids</code>               (<code>list[int]</code>)           \u2013            <p>True token value for each position in <code>masked_lm_position</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>ndarray[Any, Any]</code>           \u2013            <p>List with length <code>seq_len</code> with the true value for each corresponding         masked token in <code>input_ids</code> and -100 for all tokens in input_ids         which are not masked.</p> </li> </ul> Source code in <code>llm/datasets/bert.py</code> <pre><code>def get_masked_labels(\n    seq_len: int,\n    masked_lm_positions: list[int],\n    masked_lm_ids: list[int],\n) -&gt; np.ndarray[Any, Any]:\n    \"\"\"Create masked labels array.\n\n    Args:\n        seq_len: Sequence length.\n        masked_lm_positions: Index in sequence of masked tokens\n        masked_lm_ids: True token value for each position in\n            `masked_lm_position`.\n\n    Returns:\n        List with length `seq_len` with the true value for each corresponding \\\n        masked token in `input_ids` and -100 for all tokens in input_ids \\\n        which are not masked.\n    \"\"\"\n    masked_lm_labels = np.ones(seq_len, dtype=np.int64) * -100\n    if len(masked_lm_positions) &gt; 0:\n        # store number of  masked tokens in index\n        (padded_mask_indices,) = np.nonzero(np.array(masked_lm_positions) == 0)\n        if len(padded_mask_indices) != 0:\n            index = padded_mask_indices[0]\n        else:\n            index = len(masked_lm_positions)\n        masked_lm_labels[masked_lm_positions[:index]] = masked_lm_ids[:index]\n    return masked_lm_labels\n</code></pre>"},{"location":"api/datasets/bert/#llm.datasets.bert.get_dataloader_from_nvidia_bert_shard","title":"get_dataloader_from_nvidia_bert_shard","text":"<pre><code>get_dataloader_from_nvidia_bert_shard(\n    input_file: str,\n    batch_size: int,\n    *,\n    num_replicas: int | None = None,\n    rank: int | None = None,\n    seed: int = 0,\n    num_workers: int = 4\n) -&gt; DataLoader[Batch]\n</code></pre> <p>Create a dataloader from a dataset shard.</p> <p>Parameters:</p> <ul> <li> <code>input_file</code>               (<code>str</code>)           \u2013            <p>HDF5 file to load.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Size of batches yielded by the dataloader.</p> </li> <li> <code>num_replicas</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of processes participating in distributed training.</p> </li> <li> <code>rank</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Rank of the current process within <code>num_replicas</code>.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed used to shuffle the sampler.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of subprocesses to use for data loading.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataLoader[Batch]</code>           \u2013            <p>Dataloader which can be iterated over to yield         <code>Batch</code>.</p> </li> </ul> Source code in <code>llm/datasets/bert.py</code> <pre><code>def get_dataloader_from_nvidia_bert_shard(\n    input_file: str,\n    batch_size: int,\n    *,\n    num_replicas: int | None = None,\n    rank: int | None = None,\n    seed: int = 0,\n    num_workers: int = 4,\n) -&gt; DataLoader[Batch]:\n    \"\"\"Create a dataloader from a dataset shard.\n\n    Args:\n        input_file: HDF5 file to load.\n        batch_size: Size of batches yielded by the dataloader.\n        num_replicas: Number of processes participating in distributed\n            training.\n        rank: Rank of the current process within `num_replicas`.\n        seed: Random seed used to shuffle the sampler.\n        num_workers: Number of subprocesses to use for data loading.\n\n    Returns:\n        Dataloader which can be iterated over to yield \\\n        [`Batch`][llm.datasets.bert.Batch].\n    \"\"\"\n    dataset = NvidiaBertDataset(input_file)\n    sampler: DistributedSampler[int] = DistributedSampler(\n        dataset,\n        num_replicas=num_replicas,\n        rank=rank,\n        shuffle=True,\n        seed=seed,\n    )\n    dataloader = DataLoader(\n        dataset,\n        sampler=sampler,\n        batch_size=batch_size,\n        num_workers=4,\n        worker_init_fn=WorkerInitObj(seed),\n        pin_memory=True,\n    )\n\n    return dataloader\n</code></pre>"},{"location":"api/datasets/bert/#llm.datasets.bert.sharded_nvidia_bert_dataset","title":"sharded_nvidia_bert_dataset","text":"<pre><code>sharded_nvidia_bert_dataset(\n    input_dir: str,\n    batch_size: int,\n    *,\n    num_replicas: int | None = None,\n    rank: int | None = None,\n    seed: int = 0,\n    num_workers: int = 4\n) -&gt; Generator[Batch, None, None]\n</code></pre> <p>Simple generator which yields pretraining batches.</p> <p>Parameters:</p> <ul> <li> <code>input_dir</code>               (<code>str</code>)           \u2013            <p>Directory of HDF5 shards to load samples from.</p> </li> <li> <code>batch_size</code>               (<code>int</code>)           \u2013            <p>Size of batches yielded.</p> </li> <li> <code>num_replicas</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Number of processes participating in distributed training.</p> </li> <li> <code>rank</code>               (<code>int | None</code>, default:                   <code>None</code> )           \u2013            <p>Rank of the current process within <code>num_replicas</code>.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Random seed used to shuffle the sampler.</p> </li> <li> <code>num_workers</code>               (<code>int</code>, default:                   <code>4</code> )           \u2013            <p>Number of subprocesses to use for data loading.</p> </li> </ul> <p>Yields:</p> <ul> <li> <code>Batch</code>           \u2013            <p>Batches of pretraining data.</p> </li> </ul> Source code in <code>llm/datasets/bert.py</code> <pre><code>def sharded_nvidia_bert_dataset(\n    input_dir: str,\n    batch_size: int,\n    *,\n    num_replicas: int | None = None,\n    rank: int | None = None,\n    seed: int = 0,\n    num_workers: int = 4,\n) -&gt; Generator[Batch, None, None]:\n    \"\"\"Simple generator which yields pretraining batches.\n\n    Args:\n        input_dir: Directory of HDF5 shards to load samples from.\n        batch_size: Size of batches yielded.\n        num_replicas: Number of processes participating in distributed\n            training.\n        rank: Rank of the current process within `num_replicas`.\n        seed: Random seed used to shuffle the sampler.\n        num_workers: Number of subprocesses to use for data loading.\n\n    Yields:\n        Batches of pretraining data.\n    \"\"\"\n    shard_filepaths = get_filepaths(\n        input_dir,\n        extensions=['.h5', '.hdf5'],\n        recursive=True,\n    )\n    shard_filepaths.sort()\n\n    for shard_filepath in shard_filepaths:\n        dataloader = get_dataloader_from_nvidia_bert_shard(\n            shard_filepath,\n            batch_size,\n            num_replicas=num_replicas,\n            rank=rank,\n            seed=seed,\n        )\n        for batch in dataloader:\n            yield Batch(*batch)\n</code></pre>"},{"location":"api/datasets/roberta/","title":"llm.datasets.roberta","text":"<code>llm/datasets/roberta.py</code> <p>Custom RoBERTa dataset provider.</p> <p>This is designed to work with data produced by the RoBERTa encoder preprocessing script in <code>llm.preprocess.roberta</code>.</p>"},{"location":"api/datasets/roberta/#llm.datasets.roberta.RoBERTaDataset","title":"RoBERTaDataset","text":"<pre><code>RoBERTaDataset(\n    input_file: Path | str,\n    mask_token_id: int,\n    mask_token_prob: float,\n    vocab_size: int,\n)\n</code></pre> <p>               Bases: <code>Dataset[Sample]</code></p> <p>RoBERTa pretraining dataset.</p> <p>Like the PyTorch <code>Dataset</code>, this dataset is indexable returning a <code>Sample</code>.</p> <p>Samples are randomly masked as runtime using the provided parameters. Next sentence prediction is not supported.</p> Example <pre><code>&gt;&gt;&gt; from llm.datasets.roberta import RoBERTaDataset\n&gt;&gt;&gt; dataset = RoBERTaDataset('/path/to/shard')\n&gt;&gt;&gt; dataset[5]\nSample(...)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>input_file</code>               (<code>Path | str</code>)           \u2013            <p>HDF5 file to load.</p> </li> <li> <code>mask_token_id</code>               (<code>int</code>)           \u2013            <p>ID of the mask token in the vocabulary.</p> </li> <li> <code>mask_token_prob</code>               (<code>float</code>)           \u2013            <p>Probability of a given token in the sample being masked.</p> </li> <li> <code>vocab_size</code>               (<code>int</code>)           \u2013            <p>Size of the vocabulary. Used to replace masked tokens with a random token 10% of the time.</p> </li> </ul> Source code in <code>llm/datasets/roberta.py</code> <pre><code>def __init__(\n    self,\n    input_file: pathlib.Path | str,\n    mask_token_id: int,\n    mask_token_prob: float,\n    vocab_size: int,\n) -&gt; None:\n    self.input_file = input_file\n    self.mask_token_id = mask_token_id\n    self.mask_token_prob = mask_token_prob\n    self.vocab_size = vocab_size\n\n    self.loaded = False\n    with h5py.File(self.input_file, 'r') as f:\n        self.samples = len(f['input_ids'])\n</code></pre>"},{"location":"api/datasets/roberta/#llm.datasets.roberta.bert_mask_sequence","title":"bert_mask_sequence","text":"<pre><code>bert_mask_sequence(\n    token_ids: LongTensor,\n    special_tokens_mask: BoolTensor,\n    mask_token_id: int,\n    mask_token_prob: float,\n    vocab_size: int,\n) -&gt; tuple[LongTensor, LongTensor]\n</code></pre> <p>Randomly mask a BERT training sequence.</p> <p>Source: <code>transformers/data/data_collator.py</code></p> <p>Parameters:</p> <ul> <li> <code>token_ids</code>               (<code>LongTensor</code>)           \u2013            <p>Input sequence token IDs to mask.</p> </li> <li> <code>special_tokens_mask</code>               (<code>BoolTensor</code>)           \u2013            <p>Mask of special tokens in the sequence which should never be masked.</p> </li> <li> <code>mask_token_id</code>               (<code>int</code>)           \u2013            <p>ID of the mask token in the vocabulary.</p> </li> <li> <code>mask_token_prob</code>               (<code>float</code>)           \u2013            <p>Probability of a given token in the sample being masked.</p> </li> <li> <code>vocab_size</code>               (<code>int</code>)           \u2013            <p>Size of the vocabulary. Used to replace masked tokens with a random token 10% of the time.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[LongTensor, LongTensor]</code>           \u2013            <p>Masked <code>token_ids</code> and the masked labels.</p> </li> </ul> Source code in <code>llm/datasets/roberta.py</code> <pre><code>def bert_mask_sequence(\n    token_ids: torch.LongTensor,\n    special_tokens_mask: torch.BoolTensor,\n    mask_token_id: int,\n    mask_token_prob: float,\n    vocab_size: int,\n) -&gt; tuple[torch.LongTensor, torch.LongTensor]:\n    \"\"\"Randomly mask a BERT training sequence.\n\n    Source: [`transformers/data/data_collator.py`](https://github.com/huggingface/transformers/blob/f7329751fe5c43365751951502c00df5a4654359/src/transformers/data/data_collator.py#L748){target=_blank}\n\n    Args:\n        token_ids: Input sequence token IDs to mask.\n        special_tokens_mask: Mask of special tokens in the sequence which\n            should never be masked.\n        mask_token_id: ID of the mask token in the vocabulary.\n        mask_token_prob: Probability of a given token in the sample being\n            masked.\n        vocab_size: Size of the vocabulary. Used to replace masked tokens with\n            a random token 10% of the time.\n\n    Returns:\n        Masked `token_ids` and the masked labels.\n    \"\"\"\n    masked_labels = cast(torch.LongTensor, token_ids.clone())\n\n    probability_matrix = torch.full(token_ids.shape, mask_token_prob)\n    special_tokens_mask = special_tokens_mask.bool()\n    probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n    masked_indices = torch.bernoulli(probability_matrix).bool()\n\n    # Set non-masked tokens to -100 so loss is only computed on masked tokens\n    masked_labels[~masked_indices] = -100\n\n    # 80% of the time replace masked token with [MASK]\n    indices_replaced = (\n        torch.bernoulli(torch.full(token_ids.shape, 0.8)).bool()\n        &amp; masked_indices\n    )\n    token_ids[indices_replaced] = mask_token_id\n\n    # 10% of the time replace masked tokens with random token\n    indices_random = (\n        torch.bernoulli(torch.full(token_ids.shape, 0.5)).bool()\n        &amp; masked_indices\n        &amp; ~indices_replaced\n    )\n    random_words = torch.randint(vocab_size, token_ids.shape, dtype=torch.long)\n    token_ids[indices_random] = random_words[indices_random]\n\n    # The rest of the time (10% of the time) the masked tokens are unchanged\n    return token_ids, masked_labels\n</code></pre>"},{"location":"api/datasets/sharded/","title":"llm.datasets.sharded","text":"<code>llm/datasets/sharded.py</code> <p>Utilities for training with sharded datasets.</p>"},{"location":"api/datasets/sharded/#llm.datasets.sharded.DistributedShardedDataset","title":"DistributedShardedDataset","text":"<pre><code>DistributedShardedDataset(\n    dataset_type: type[Dataset[SampleType]],\n    shard_params: dict[str, DatasetParams],\n    *,\n    rank: int,\n    world_size: int,\n    shuffle: bool = False,\n    seed: int = 0\n)\n</code></pre> <p>               Bases: <code>Dataset[SampleType]</code></p> <p>Dataset wrapper for sharded datasets in distributed environments.</p> <p>This class manages a set of datasets (shards) and restricts ranks to viewing a subset of the global indices across the shards. This is achieved by sorting the shards and counting the samples in each shard to compute the total number of samples then chunking those samples by rank.</p> <p>For example, if there are four ranks and eight shards of equal size, rank zero will see shards zero and one, rank two will see shards two and three, and so on. The length of an instance of this class as seen by a rank will be <code>(1 / world_size) * sum_of_samples_across_shards</code>.</p> <p>This class also ensures only one shard is loaded at a time on a rank so the full dataset is never loaded into memory at once.</p> Warning <p>When building a <code>DataLoader</code> from a <code>DistributedShardedDataset</code>, do NOT use PyTorch's <code>DistributedSampler</code>. If you want to be able to save the state of the data loader, use the <code>SequentialSampler</code> because this class already provides the support for partitioning samples across ranks. This module provides a <code>ResumableSequentialSampler</code> to enable resuming sampling from the last sampled index.</p> Note <p>Samples at the end of the last shard will be dropped to ensure each rank sees an equal number of samples.</p> Todo <ul> <li>Next shard prefetching</li> <li>Sample index shuffling within a shard</li> <li>Support shuffle shard order by epoch</li> </ul> <p>Parameters:</p> <ul> <li> <code>dataset_type</code>               (<code>type[Dataset[SampleType]]</code>)           \u2013            <p>Dataset type that represents a single shard. This subtype of Dataset must be a map-style dataset. Iterable-style datasets are not supported.</p> </li> <li> <code>shard_params</code>               (<code>dict[str, DatasetParams]</code>)           \u2013            <p>Dictionary mapping shard keys to the parameters used to initialize a <code>dataset_type</code> for the shard. The parameter type is a tuple of args and kwargs.</p> </li> <li> <code>rank</code>               (<code>int</code>)           \u2013            <p>Rank of this process.</p> </li> <li> <code>world_size</code>               (<code>int</code>)           \u2013            <p>Number of ranks sharing the dataset.</p> </li> <li> <code>shuffle</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Shuffle the shard order by the shard keys. The default (<code>False</code>) sorts the shards by shard key.</p> </li> <li> <code>seed</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Seed used for shuffling the shard order.</p> </li> </ul> Source code in <code>llm/datasets/sharded.py</code> <pre><code>def __init__(\n    self,\n    dataset_type: type[Dataset[SampleType]],\n    shard_params: dict[str, DatasetParams],\n    *,\n    rank: int,\n    world_size: int,\n    shuffle: bool = False,\n    seed: int = 0,\n) -&gt; None:\n    if not (0 &lt;= rank &lt; world_size):\n        raise ValueError(\n            f'Got rank={rank} which does not satisfy 0 &lt;= rank &lt; '\n            f'world_size where world_size={world_size}.',\n        )\n    if len(shard_params) == 0:\n        raise ValueError(\n            'Parameters for at least one shard must be provided.',\n        )\n\n    random.seed(seed)\n\n    self.dataset_type = dataset_type\n    self.shard_params = shard_params\n    self.rank = rank\n    self.world_size = world_size\n    self.shuffle = shuffle\n\n    shard_keys = sorted(shard_params.keys())\n    if shuffle:\n        random.shuffle(shard_keys)\n\n    # Mapping of shard_key to (start_index, end_index)\n    shard_indices: dict[str, tuple[int, int]] = {}\n    index = 0\n    for shard_key in shard_keys:\n        shard = self.load_shard(shard_key)\n        assert isinstance(shard, Sized)\n        shard_indices[shard_key] = (index, index + len(shard))\n        index += len(shard)\n        del shard\n\n    # Drop indices from last shard to make divisible by world size\n    last_shard_key = shard_keys[-1]\n    last_shard_indices = shard_indices[last_shard_key]\n    shard_indices[last_shard_key] = (\n        last_shard_indices[0],\n        last_shard_indices[1] - (last_shard_indices[1] % world_size),\n    )\n\n    self.shard_keys = shard_keys\n    self.shard_indices = shard_indices\n    self.total_samples = shard_indices[last_shard_key][1]\n\n    assert len(shard_keys) == len(shard_indices) == len(shard_params)\n    assert len(self) * self.world_size == self.total_samples\n\n    self._current_shard_key: str | None = None\n    self._current_shard: Dataset[SampleType] | None = None\n</code></pre>"},{"location":"api/datasets/sharded/#llm.datasets.sharded.DistributedShardedDataset.rank_index_to_global_index","title":"rank_index_to_global_index","text":"<pre><code>rank_index_to_global_index(rank_index: int) -&gt; int\n</code></pre> <p>Convert an index local to a rank to a global index.</p> Source code in <code>llm/datasets/sharded.py</code> <pre><code>def rank_index_to_global_index(self, rank_index: int) -&gt; int:\n    \"\"\"Convert an index local to a rank to a global index.\"\"\"\n    rank_start_index = len(self) * self.rank\n    return rank_start_index + rank_index\n</code></pre>"},{"location":"api/datasets/sharded/#llm.datasets.sharded.DistributedShardedDataset.rank_index_to_shard_index","title":"rank_index_to_shard_index","text":"<pre><code>rank_index_to_shard_index(\n    rank_index: int,\n) -&gt; tuple[str, int]\n</code></pre> <p>Convert an index local to a rank to a shard and shard index.</p> <p>Parameters:</p> <ul> <li> <code>rank_index</code>               (<code>int</code>)           \u2013            <p>Dataset index local to the rank.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[str, int]</code>           \u2013            <p>Tuple of the shard key and the index within the shard that             <code>rank_index</code> corresponds to.</p> </li> </ul> Source code in <code>llm/datasets/sharded.py</code> <pre><code>def rank_index_to_shard_index(self, rank_index: int) -&gt; tuple[str, int]:\n    \"\"\"Convert an index local to a rank to a shard and shard index.\n\n    Args:\n        rank_index: Dataset index local to the rank.\n\n    Returns:\n        Tuple of the shard key and the index within the shard that \\\n        `rank_index` corresponds to.\n    \"\"\"\n    global_index = self.rank_index_to_global_index(rank_index)\n    for shard_key in self.shard_keys:\n        shard_indices = self.shard_indices[shard_key]\n        if shard_indices[0] &lt;= global_index &lt; shard_indices[1]:\n            return (shard_key, global_index - shard_indices[0])\n    raise AssertionError(\n        f'Rank index {rank_index} for rank {self.rank} maps to global '\n        f'index {global_index} which exceeds the total samples in the '\n        f'dataset ({self.total_samples}).',\n    )\n</code></pre>"},{"location":"api/datasets/sharded/#llm.datasets.sharded.ResumableSequentialSampler","title":"ResumableSequentialSampler","text":"<pre><code>ResumableSequentialSampler(\n    data_source: Sized, start_index: int = 0\n)\n</code></pre> <p>               Bases: <code>Sampler[int]</code></p> <p>Resumable sequential sampler.</p> <p>Parameters:</p> <ul> <li> <code>data_source</code>               (<code>Sized</code>)           \u2013            <p>Dataset to sample sequentially from.</p> </li> <li> <code>start_index</code>               (<code>int</code>, default:                   <code>0</code> )           \u2013            <p>Index to resume sequential sampling from.</p> </li> </ul> Source code in <code>llm/datasets/sharded.py</code> <pre><code>def __init__(self, data_source: Sized, start_index: int = 0) -&gt; None:\n    self.data_length = len(data_source)\n    self.start_index = start_index\n    self.index = start_index\n</code></pre>"},{"location":"api/engine/","title":"llm.engine","text":"<code>llm/engine/__init__.py</code> <p>Training engine utilities.</p> <p>This module provides wrappers for models, optimizers, and more to enable efficient training with mixed precision, distributed data parallelism, and more.</p>"},{"location":"api/engine/accumulation/","title":"llm.engine.accumulation","text":"<code>llm/engine/accumulation.py</code> <p>Utilities for easy gradient accumulation training.</p>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer","title":"GradientAccumulationOptimizer","text":"<pre><code>GradientAccumulationOptimizer(\n    optimizer: BaseOptimizer,\n    model: Module,\n    accumulation_steps: int,\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Optimizer wrapper for enabling gradient accumulation.</p> <p>This wrapper will skip calls to <code>BaseOptimizer.step()</code> until <code>accumulation_steps</code> forward/backward passes have been performed.</p> <p>Parameters:</p> <ul> <li> <code>optimizer</code>               (<code>BaseOptimizer</code>)           \u2013            <p>Optimizer to wrap.</p> </li> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Model being optimized.</p> </li> <li> <code>accumulation_steps</code>               (<code>int</code>)           \u2013            <p>Number of iterations between optimization steps.</p> </li> </ul> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def __init__(\n    self,\n    optimizer: BaseOptimizer,\n    model: torch.nn.Module,\n    accumulation_steps: int,\n) -&gt; None:\n    super().__init__(optimizer)\n    self._accumulation_steps = accumulation_steps\n    self._accumulation_step = 0\n    self._model = model\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer.accumulation_boundary","title":"accumulation_boundary","text":"<pre><code>accumulation_boundary() -&gt; bool\n</code></pre> <p>Return if the current step is an accumulation boundary.</p> <p>I.e., the last call to <code>step()</code> resulted in an optimization step and no accumulation for the next step has started.</p> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def accumulation_boundary(self) -&gt; bool:\n    \"\"\"Return if the current step is an accumulation boundary.\n\n    I.e., the last call to\n    [`step()`][llm.engine.accumulation.GradientAccumulationOptimizer.step]\n    resulted in an optimization step and no accumulation for the next step\n    has started.\n    \"\"\"\n    return self._accumulation_step == 0\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer.backward","title":"backward","text":"<pre><code>backward(loss: Tensor) -&gt; None\n</code></pre> <p>Perform a backward pass.</p> Note <p>If <code>model</code> is a <code>DistributedDataParallel</code> instance, backward passes will be performed with <code>no_sync()</code> during gradient accumulation steps.</p> <p>Parameters:</p> <ul> <li> <code>loss</code>               (<code>Tensor</code>)           \u2013            <p>Loss to compute gradients with respect to.</p> </li> </ul> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def backward(self, loss: torch.Tensor) -&gt; None:\n    \"\"\"Perform a backward pass.\n\n    Note:\n        If `model` is a\n        [`DistributedDataParallel`][torch.nn.parallel.DistributedDataParallel]\n        instance, backward passes will be performed with\n        [`no_sync()`][torch.nn.parallel.DistributedDataParallel.no_sync]\n        during gradient accumulation steps.\n\n    Args:\n        loss: Loss to compute gradients with respect to.\n    \"\"\"\n    self._accumulation_step += 1\n\n    context = (\n        self._model.no_sync()\n        if (\n            self._accumulation_step &lt; self._accumulation_steps\n            and isinstance(self._model, DistributedDataParallel)\n        )\n        else contextlib.nullcontext()\n    )\n\n    with context:\n        scaled_loss = loss / self._accumulation_steps\n        self._optimizer.backward(scaled_loss)\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer.step","title":"step","text":"<pre><code>step(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Perform an optimization step.</p> <p>This method is a no-op unless <code>accumulation_steps</code> have occurred.</p> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def step(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Perform an optimization step.\n\n    This method is a no-op unless `accumulation_steps` have occurred.\n    \"\"\"\n    if self._accumulation_step == self._accumulation_steps:\n        self._accumulation_step = 0\n        self._optimizer.step(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationOptimizer.zero_grad","title":"zero_grad","text":"<pre><code>zero_grad(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Zero the gradients of the wrapped optimizer.</p> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def zero_grad(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Zero the gradients of the wrapped optimizer.\"\"\"\n    if self._accumulation_step == 0:\n        self._optimizer.zero_grad(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationLRScheduler","title":"GradientAccumulationLRScheduler","text":"<pre><code>GradientAccumulationLRScheduler(\n    scheduler: _LRScheduler, accumulation_steps: int\n)\n</code></pre> <p>               Bases: <code>_LRScheduler</code></p> <p>LR scheduler wrapper that accounts for gradient accumulation.</p> <p>This wrapper allows you to call <code>scheduler.step()</code> after every forward/backward pass and will correctly skip the call if it happens during a gradient accumulation period.</p> <p>Parameters:</p> <ul> <li> <code>scheduler</code>               (<code>_LRScheduler</code>)           \u2013            <p>LR scheduler to wrap.</p> </li> <li> <code>accumulation_steps</code>               (<code>int</code>)           \u2013            <p>Number of iterations between optimization steps.</p> </li> </ul> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def __init__(\n    self,\n    scheduler: _LRScheduler,\n    accumulation_steps: int,\n) -&gt; None:\n    self._accumulation_steps = accumulation_steps\n    self._accumulation_step = 0\n    self._scheduler = scheduler\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.GradientAccumulationLRScheduler.step","title":"step","text":"<pre><code>step(epoch: int | None = None) -&gt; None\n</code></pre> <p>Update the learning rate.</p> <p>This method is a no-op unless <code>accumulation_steps</code> have occurred.</p> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def step(self, epoch: int | None = None) -&gt; None:\n    \"\"\"Update the learning rate.\n\n    This method is a no-op unless `accumulation_steps` have occurred.\n    \"\"\"\n    self._accumulation_step += 1\n    if self._accumulation_step == self._accumulation_steps:\n        self._accumulation_step = 0\n        self._scheduler.step(epoch)\n</code></pre>"},{"location":"api/engine/accumulation/#llm.engine.accumulation.initialize","title":"initialize","text":"<pre><code>initialize(\n    model: Module,\n    optimizer: BaseOptimizer,\n    scheduler: _LRScheduler,\n    accumulation_steps: int = 1,\n) -&gt; tuple[\n    GradientAccumulationOptimizer,\n    GradientAccumulationLRScheduler,\n]\n</code></pre> <p>Initialize gradient accumulation training.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Model being optimized.</p> </li> <li> <code>optimizer</code>               (<code>BaseOptimizer</code>)           \u2013            <p>Optimizer to wrap.</p> </li> <li> <code>scheduler</code>               (<code>_LRScheduler</code>)           \u2013            <p>LR scheduler to wrap.</p> </li> <li> <code>accumulation_steps</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of iterations between optimization steps.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[GradientAccumulationOptimizer, GradientAccumulationLRScheduler]</code>           \u2013            <p>The wrapped optimizer and LR scheduler.</p> </li> </ul> Source code in <code>llm/engine/accumulation.py</code> <pre><code>def initialize(\n    model: torch.nn.Module,\n    optimizer: BaseOptimizer,\n    scheduler: _LRScheduler,\n    accumulation_steps: int = 1,\n) -&gt; tuple[GradientAccumulationOptimizer, GradientAccumulationLRScheduler]:\n    \"\"\"Initialize gradient accumulation training.\n\n    Args:\n        model: Model being optimized.\n        optimizer: Optimizer to wrap.\n        scheduler: LR scheduler to wrap.\n        accumulation_steps: Number of iterations between optimization steps.\n\n    Returns:\n        The wrapped optimizer and LR scheduler.\n    \"\"\"\n    return (\n        GradientAccumulationOptimizer(optimizer, model, accumulation_steps),\n        GradientAccumulationLRScheduler(scheduler, accumulation_steps),\n    )\n</code></pre>"},{"location":"api/engine/amp/","title":"llm.engine.amp","text":"<code>llm/engine/amp.py</code> <p>Utilities for easy automatic mixed precision training.</p>"},{"location":"api/engine/amp/#llm.engine.amp.AMPCriterion","title":"AMPCriterion","text":"<pre><code>AMPCriterion(criterion: Module, autocast: autocast)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Wrap a loss function for AMP training.</p> <p>Parameters:</p> <ul> <li> <code>criterion</code>               (<code>Module</code>)           \u2013            <p>Loss function to wrap.</p> </li> <li> <code>autocast</code>               (<code>autocast</code>)           \u2013            <p>Autocast context manager to compute loss inside.</p> </li> </ul> Source code in <code>llm/engine/amp.py</code> <pre><code>def __init__(\n    self,\n    criterion: torch.nn.Module,\n    autocast: torch.autocast,\n) -&gt; None:\n    super().__init__()\n    self._criterion = criterion\n    self._autocast = autocast\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPCriterion.forward","title":"forward","text":"<pre><code>forward(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Compute the loss inside the autocast.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Compute the loss inside the autocast.\"\"\"\n    with self._autocast:\n        return self._criterion(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPModel","title":"AMPModel","text":"<pre><code>AMPModel(model: Module, autocast: autocast)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Wrap a model for AMP training.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Model to wrap.</p> </li> <li> <code>autocast</code>               (<code>autocast</code>)           \u2013            <p>Autocast context manager to compute loss inside.</p> </li> </ul> Source code in <code>llm/engine/amp.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    autocast: torch.autocast,\n) -&gt; None:\n    super().__init__()\n    self._model = model\n    self._autocast = autocast\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPModel.forward","title":"forward","text":"<pre><code>forward(*args: Any, **kwargs: Any) -&gt; Any\n</code></pre> <p>Perform a forward pass inside the autocast.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def forward(self, *args: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Perform a forward pass inside the autocast.\"\"\"\n    with self._autocast:\n        return self._model(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer","title":"AMPOptimizer","text":"<pre><code>AMPOptimizer(\n    model: Module,\n    optimizer: Optimizer,\n    scaler: GradScaler,\n    max_norm: float | None = None,\n)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code></p> <p>Wrap an optimizer for AMP training.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Model being optimized.</p> </li> <li> <code>optimizer</code>               (<code>Optimizer</code>)           \u2013            <p>Optimizer to wrap.</p> </li> <li> <code>scaler</code>               (<code>GradScaler</code>)           \u2013            <p>Gradient scaler.</p> </li> <li> <code>max_norm</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Optionally clip gradient norm.</p> </li> </ul> Source code in <code>llm/engine/amp.py</code> <pre><code>def __init__(\n    self,\n    model: torch.nn.Module,\n    optimizer: Optimizer,\n    scaler: GradScaler,\n    max_norm: float | None = None,\n) -&gt; None:\n    super().__init__(optimizer)\n    self._model = model\n    self._scaler = scaler\n    self._max_norm = max_norm\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer.zero_grad","title":"zero_grad","text":"<pre><code>zero_grad(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Zero the gradients of optimized parameters.</p> Source code in <code>llm/engine/base.py</code> <pre><code>def zero_grad(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Zero the gradients of optimized parameters.\"\"\"\n    self._optimizer.zero_grad(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer.state_dict","title":"state_dict","text":"<pre><code>state_dict() -&gt; dict[str, Any]\n</code></pre> <p>Dictionary containing references to the whole state of the module.</p> <p>Includes the state of the <code>grad_scaler</code>.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def state_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Dictionary containing references to the whole state of the module.\n\n    Includes the state of the `grad_scaler`.\n    \"\"\"\n    state_dict = self._optimizer.state_dict()\n    assert 'grad_scaler' not in state_dict\n    state_dict['grad_scaler'] = self._scaler.state_dict()\n    return state_dict\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer.load_state_dict","title":"load_state_dict","text":"<pre><code>load_state_dict(state_dict: dict[str, Any]) -&gt; None\n</code></pre> <p>Copy the state into this module.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def load_state_dict(self, state_dict: dict[str, Any]) -&gt; None:\n    \"\"\"Copy the state into this module.\"\"\"\n    scaler_state_dict = state_dict.pop('grad_scaler', None)\n    if scaler_state_dict is not None:  # pragma: no branch\n        self._scaler.load_state_dict(scaler_state_dict)\n    self._optimizer.load_state_dict(state_dict)\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer.backward","title":"backward","text":"<pre><code>backward(loss: Tensor) -&gt; None\n</code></pre> <p>Perform a backward pass and correctly scale the loss.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def backward(self, loss: torch.Tensor) -&gt; None:\n    \"\"\"Perform a backward pass and correctly scale the loss.\"\"\"\n    self._scaler.scale(loss).backward()\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.AMPOptimizer.step","title":"step","text":"<pre><code>step(closure: Callable[[], float] | None = None) -&gt; None\n</code></pre> <p>Perform an optimization using the gradient scaler.</p> Source code in <code>llm/engine/amp.py</code> <pre><code>def step(self, closure: Callable[[], float] | None = None) -&gt; None:\n    \"\"\"Perform an optimization using the gradient scaler.\"\"\"\n    if self._max_norm is not None:\n        self._scaler.unscale_(self._optimizer)\n        torch.nn.utils.clip_grad_norm_(\n            self._model.parameters(),\n            self._max_norm,\n        )\n    self._scaler.step(self._optimizer)\n    self._scaler.update()\n</code></pre>"},{"location":"api/engine/amp/#llm.engine.amp.initialize","title":"initialize","text":"<pre><code>initialize(\n    model: Module,\n    optimizer: Optimizer,\n    criterion: Module,\n    dtype: dtype = torch.float16,\n    max_norm: float | None = None,\n    **kwargs: Any\n) -&gt; tuple[AMPModel, AMPOptimizer, AMPCriterion]\n</code></pre> <p>Initialize AMP training.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Model being optimized.</p> </li> <li> <code>optimizer</code>               (<code>Optimizer</code>)           \u2013            <p>Optimizer to wrap.</p> </li> <li> <code>criterion</code>               (<code>Module</code>)           \u2013            <p>Loss function to wrap.</p> </li> <li> <code>dtype</code>               (<code>dtype</code>, default:                   <code>float16</code> )           \u2013            <p>Data type to perform mixed precision in. Typically <code>torch.float16</code> or <code>torch.bfloat16</code>.</p> </li> <li> <code>max_norm</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Optionally clip gradient norm.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional keyword arguments to pass to the <code>GradScaler</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[AMPModel, AMPOptimizer, AMPCriterion]</code>           \u2013            <p>A tuple of the wrapped model, optimizer, and loss.</p> </li> </ul> Source code in <code>llm/engine/amp.py</code> <pre><code>def initialize(\n    model: torch.nn.Module,\n    optimizer: Optimizer,\n    criterion: torch.nn.Module,\n    dtype: torch.dtype = torch.float16,\n    max_norm: float | None = None,\n    **kwargs: Any,\n) -&gt; tuple[AMPModel, AMPOptimizer, AMPCriterion]:\n    \"\"\"Initialize AMP training.\n\n    Args:\n        model: Model being optimized.\n        optimizer: Optimizer to wrap.\n        criterion: Loss function to wrap.\n        dtype: Data type to perform mixed precision in. Typically\n            `torch.float16` or `torch.bfloat16`.\n        max_norm: Optionally clip gradient norm.\n        kwargs: Additional keyword arguments to pass to the\n            [`GradScaler`][torch.cuda.amp.GradScaler].\n\n    Returns:\n        A tuple of the wrapped model, optimizer, and loss.\n    \"\"\"\n    device = 'cuda' if next(model.parameters()).is_cuda else 'cpu'\n    autocast = torch.autocast(device, dtype=dtype)\n\n    # GradScaler only works on CUDA tensors so we disable on CPU\n    scaler = GradScaler(**kwargs, enabled=device == 'cuda')\n\n    return (\n        AMPModel(model, autocast),\n        AMPOptimizer(model, optimizer, scaler, max_norm),\n        AMPCriterion(criterion, autocast),\n    )\n</code></pre>"},{"location":"api/engine/base/","title":"llm.engine.base","text":"<code>llm/engine/base.py</code>"},{"location":"api/engine/base/#llm.engine.base.BaseOptimizer","title":"BaseOptimizer","text":"<pre><code>BaseOptimizer(optimizer: Optimizer)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> <p>Base optimizer wrapper.</p> <p>Compared to a standard PyTorch <code>Optimizer</code>, this optimizer adds the <code>backward()</code> that is used instead of directly calling <code>loss.backward()</code>. This is needed so the various optimizer wrappers can adjust how the loss is computed.</p> <p>Otherwise, this optimizer behaves identically to the wrapped optimizer.</p> <p>Parameters:</p> <ul> <li> <code>optimizer</code>               (<code>Optimizer</code>)           \u2013            <p>Standard PyTorch optimizer to wrap.</p> </li> </ul> Source code in <code>llm/engine/base.py</code> <pre><code>def __init__(self, optimizer: Optimizer):\n    self._optimizer = optimizer\n</code></pre>"},{"location":"api/engine/base/#llm.engine.base.BaseOptimizer.step","title":"step","text":"<pre><code>step(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Perform an optimization step.</p> Source code in <code>llm/engine/base.py</code> <pre><code>def step(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Perform an optimization step.\"\"\"\n    self._optimizer.step(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/base/#llm.engine.base.BaseOptimizer.zero_grad","title":"zero_grad","text":"<pre><code>zero_grad(*args: Any, **kwargs: Any) -&gt; None\n</code></pre> <p>Zero the gradients of optimized parameters.</p> Source code in <code>llm/engine/base.py</code> <pre><code>def zero_grad(self, *args: Any, **kwargs: Any) -&gt; None:\n    \"\"\"Zero the gradients of optimized parameters.\"\"\"\n    self._optimizer.zero_grad(*args, **kwargs)\n</code></pre>"},{"location":"api/engine/base/#llm.engine.base.BaseOptimizer.backward","title":"backward","text":"<pre><code>backward(loss: Tensor) -&gt; None\n</code></pre> <p>Perform a backward pass using the loss.</p> Source code in <code>llm/engine/base.py</code> <pre><code>def backward(self, loss: torch.Tensor) -&gt; None:\n    \"\"\"Perform a backward pass using the loss.\"\"\"\n    loss.backward()\n</code></pre>"},{"location":"api/engine/initialize/","title":"llm.engine.initialize","text":"<code>llm/engine/initialize.py</code>"},{"location":"api/engine/initialize/#llm.engine.initialize.initialize","title":"initialize","text":"<pre><code>initialize(\n    model: Module,\n    optimizer: Optimizer,\n    criterion: Module,\n    scheduler: _LRScheduler,\n    accumulation_steps: int = 1,\n    dtype: dtype | None = None,\n    max_norm: float | None = None,\n    **kwargs: Any\n) -&gt; tuple[Module, BaseOptimizer, Module, _LRScheduler]\n</code></pre> <p>Enable advanced training features.</p> <p>This method allows you to easily wrap your training objects with transparent wrappers that enable advanced training features.</p> <p>Parameters:</p> <ul> <li> <code>model</code>               (<code>Module</code>)           \u2013            <p>Model being trained.</p> </li> <li> <code>optimizer</code>               (<code>Optimizer</code>)           \u2013            <p>Training optimizer.</p> </li> <li> <code>criterion</code>               (<code>Module</code>)           \u2013            <p>Training loss function.</p> </li> <li> <code>scheduler</code>               (<code>_LRScheduler</code>)           \u2013            <p>LR scheduler.</p> </li> <li> <code>accumulation_steps</code>               (<code>int</code>, default:                   <code>1</code> )           \u2013            <p>Number of forward/backward passes between optimizer steps.</p> </li> <li> <code>dtype</code>               (<code>dtype | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional data type for mixed precision training.</p> </li> <li> <code>max_norm</code>               (<code>float | None</code>, default:                   <code>None</code> )           \u2013            <p>Optional maximum norm of gradients to clip to.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Keyword arguments to pass to the gradient scaler.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple[Module, BaseOptimizer, Module, _LRScheduler]</code>           \u2013            <p>Tuple of the wrapped model, optimizer, loss, and scheduler.</p> </li> </ul> Source code in <code>llm/engine/initialize.py</code> <pre><code>def initialize(\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    criterion: torch.nn.Module,\n    scheduler: torch.optim.lr_scheduler._LRScheduler,\n    accumulation_steps: int = 1,\n    dtype: torch.dtype | None = None,\n    max_norm: float | None = None,\n    **kwargs: Any,\n) -&gt; tuple[\n    torch.nn.Module,\n    BaseOptimizer,\n    torch.nn.Module,\n    torch.optim.lr_scheduler._LRScheduler,\n]:\n    \"\"\"Enable advanced training features.\n\n    This method allows you to easily wrap your training objects with\n    transparent wrappers that enable advanced training features.\n\n    Args:\n        model: Model being trained.\n        optimizer: Training optimizer.\n        criterion: Training loss function.\n        scheduler: LR scheduler.\n        accumulation_steps: Number of forward/backward passes between\n            optimizer steps.\n        dtype: Optional data type for mixed precision training.\n        max_norm: Optional maximum norm of gradients to clip to.\n        kwargs: Keyword arguments to pass to the gradient scaler.\n\n    Returns:\n        Tuple of the wrapped model, optimizer, loss, and scheduler.\n    \"\"\"\n    if torch.cuda.is_available():\n        logger.debug(\n            f'Moving model to model to cuda:{torch.cuda.current_device()}.',\n            extra={'ranks': [0]},\n        )\n        model.cuda()\n\n    if torch.distributed.is_initialized():\n        local_rank = (\n            int(os.environ['LOCAL_RANK'])\n            if torch.cuda.is_available()\n            else None\n        )\n        logger.debug(\n            'Wrapping model with DistributedDataParallel with '\n            f'local_rank {local_rank}',\n            extra={'ranks': [0]},\n        )\n        model = torch.nn.parallel.DistributedDataParallel(\n            model,\n            device_ids=[local_rank] if local_rank is not None else None,\n            output_device=local_rank,\n        )\n\n    optimizer = BaseOptimizer(optimizer)\n\n    if dtype is not None:\n        logger.debug(\n            f'Initializing model for AMP training with dtype {dtype}',\n            extra={'ranks': [0]},\n        )\n        model, optimizer, criterion = amp.initialize(\n            model=model,\n            optimizer=optimizer,\n            criterion=criterion,\n            dtype=dtype,\n            max_norm=max_norm,\n            **kwargs,\n        )\n\n    if accumulation_steps &gt; 1:\n        logger.debug(\n            'Initializing model gradient accumulation steps = '\n            f'{accumulation_steps}',\n            extra={'ranks': [0]},\n        )\n        optimizer, scheduler = accumulation.initialize(\n            model=model,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            accumulation_steps=accumulation_steps,\n        )\n\n    return (model, optimizer, criterion, scheduler)\n</code></pre>"},{"location":"api/models/","title":"llm.models","text":"<code>llm/models/__init__.py</code> <p>PyTorch models.</p>"},{"location":"api/models/bert/","title":"llm.models.bert","text":"<code>llm/models/bert.py</code> <p>Utilities for loading BERT models from HuggingFace.</p> <p>Source: ColossalAI-Examples</p>"},{"location":"api/models/bert/#llm.models.bert.BERT_BASE","title":"BERT_BASE  <code>module-attribute</code>","text":"<pre><code>BERT_BASE = dict(\n    attention_probs_dropout_prob=0.1,\n    hidden_act=\"gelu_new\",\n    hidden_dropout_prob=0.1,\n    hidden_size=768,\n    initializer_range=0.02,\n    intermediate_size=3072,\n    max_position_embeddings=512,\n    num_attention_heads=12,\n    num_hidden_layers=12,\n    type_vocab_size=2,\n    vocab_size=30522,\n)\n</code></pre> <p>BERT-base HuggingFace configuration.</p>"},{"location":"api/models/bert/#llm.models.bert.BERT_LARGE","title":"BERT_LARGE  <code>module-attribute</code>","text":"<pre><code>BERT_LARGE = dict(\n    attention_probs_dropout_prob=0.1,\n    hidden_act=\"gelu_new\",\n    hidden_dropout_prob=0.1,\n    hidden_size=1024,\n    initializer_range=0.02,\n    intermediate_size=4096,\n    max_position_embeddings=512,\n    num_attention_heads=16,\n    num_hidden_layers=24,\n    type_vocab_size=2,\n    vocab_size=30522,\n)\n</code></pre> <p>BERT-large HuggingFace configuration.</p>"},{"location":"api/models/bert/#llm.models.bert.from_config","title":"from_config","text":"<pre><code>from_config(\n    config: dict[str, Any],\n    checkpoint_gradients: bool = False,\n) -&gt; BertForPreTraining\n</code></pre> <p>Load a BERT model from the configuration.</p> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>dict[str, Any]</code>)           \u2013            <p>BERT configuration.</p> </li> <li> <code>checkpoint_gradients</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Enable gradient checkpointing.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BertForPreTraining</code>           \u2013            <p>BERT model.</p> </li> </ul> Source code in <code>llm/models/bert.py</code> <pre><code>def from_config(\n    config: dict[str, Any],\n    checkpoint_gradients: bool = False,\n) -&gt; transformers.BertForPreTraining:\n    \"\"\"Load a BERT model from the configuration.\n\n    Args:\n        config: BERT configuration.\n        checkpoint_gradients: Enable gradient checkpointing.\n\n    Returns:\n        BERT model.\n    \"\"\"\n    config = transformers.BertConfig(**config)\n    model = transformers.BertForPreTraining(config)\n    if checkpoint_gradients:\n        model.gradient_checkpointing_enable()\n    else:\n        model.gradient_checkpointing_disable()\n    return model\n</code></pre>"},{"location":"api/preprocess/","title":"llm.preprocess","text":"<code>llm/preprocess/__init__.py</code> <p>Pretraining preprocessing CLIs.</p> <p>See the CLI Reference for usage instructions.</p>"},{"location":"api/preprocess/download/","title":"llm.preprocess.download","text":"<code>llm/preprocess/download.py</code> <p>Pretraining corpus downloader.</p> <pre><code>python -m llm.preprocess.download --help\n</code></pre>"},{"location":"api/preprocess/download/#llm.preprocess.download.cli","title":"cli","text":"<pre><code>cli(\n    dataset: Literal[\"wikipedia\", \"bookcorpus\"],\n    output_dir: str,\n    log_level: str,\n    rich: bool,\n) -&gt; None\n</code></pre> <p>Pretraining text downloader.</p> Source code in <code>llm/preprocess/download.py</code> <pre><code>@click.command()\n@click.option(\n    '-d',\n    '--dataset',\n    type=click.Choice(['wikipedia', 'bookscorpus'], case_sensitive=False),\n    required=True,\n    help='Dataset to download.',\n)\n@click.option(\n    '-o',\n    '--output-dir',\n    metavar='PATH',\n    required=True,\n    help='Output directory.',\n)\n@click.option(\n    '--log-level',\n    default='INFO',\n    type=click.Choice(\n        ['DEBUG', 'INFO', 'WARNING', 'ERROR'],\n        case_sensitive=False,\n    ),\n    help='Minimum logging level.',\n)\n@click.option(\n    '--rich/--no-rich',\n    default=False,\n    help='Use rich output formatting.',\n)\ndef cli(\n    dataset: Literal['wikipedia', 'bookcorpus'],\n    output_dir: str,\n    log_level: str,\n    rich: bool,\n) -&gt; None:\n    \"\"\"Pretraining text downloader.\"\"\"\n    init_logging(log_level, rich=rich)\n\n    if dataset == 'wikipedia':\n        download_wikipedia(output_dir)\n    elif dataset == 'bookscorpus':\n        download_bookscorpus(output_dir)\n    else:\n        raise AssertionError('Unreachable.')\n</code></pre>"},{"location":"api/preprocess/format/","title":"llm.preprocess.format","text":"<code>llm/preprocess/format.py</code> <p>Pretraining text formatting utilities.</p>"},{"location":"api/preprocess/format/#llm.preprocess.format.combine_document_files","title":"combine_document_files","text":"<pre><code>combine_document_files(\n    filepaths: Iterable[Path | str], output_file: Path | str\n) -&gt; None\n</code></pre> <p>Combine multiple text files into one.</p> Source code in <code>llm/preprocess/format.py</code> <pre><code>def combine_document_files(\n    filepaths: Iterable[pathlib.Path | str],\n    output_file: pathlib.Path | str,\n) -&gt; None:\n    \"\"\"Combine multiple text files into one.\"\"\"\n    output_file = pathlib.Path(output_file)\n    output_file.parent.mkdir(exist_ok=True)\n\n    sent_tokenizer = get_sent_tokenizer()\n\n    with open(output_file, 'w') as target:\n        for filepath in filepaths:\n            with open(filepath) as f:\n                document_lines = f.readlines()\n                sentences = []\n                for line in document_lines:\n                    sentences.extend(sent_tokenizer(line.strip()))\n                target.write('\\n'.join(sentences))\n                target.write('\\n\\n')\n</code></pre>"},{"location":"api/preprocess/format/#llm.preprocess.format.get_sent_tokenizer","title":"get_sent_tokenizer","text":"<pre><code>get_sent_tokenizer() -&gt; Callable[[str], list[str]]\n</code></pre> <p>Get a sentence tokenizer.</p> <p>Returns:</p> <ul> <li> <code>Callable[[str], list[str]]</code>           \u2013            <p>An NLTK sentence tokenizer.</p> </li> </ul> Source code in <code>llm/preprocess/format.py</code> <pre><code>def get_sent_tokenizer() -&gt; Callable[[str], list[str]]:\n    \"\"\"Get a sentence tokenizer.\n\n    Returns:\n        An NLTK sentence tokenizer.\n    \"\"\"\n    downloader = nltk.downloader.Downloader()\n    if not downloader.is_installed('punkt_tab'):  # pragma: no cover\n        nltk.download('punkt_tab', quiet=True)\n        download_dir = downloader.default_download_dir()\n        logger.info(f'Downloaded NLTK punkt model to {download_dir}.')\n\n    return nltk.tokenize.sent_tokenize\n</code></pre>"},{"location":"api/preprocess/format/#llm.preprocess.format.read_documents_bytes","title":"read_documents_bytes","text":"<pre><code>read_documents_bytes(\n    files: Iterable[Path | str] | Path | str,\n) -&gt; list[bytes]\n</code></pre> <p>Read documents from files.</p> <p>Parameters:</p> <ul> <li> <code>files</code>               (<code>Iterable[Path | str] | Path | str</code>)           \u2013            <p>List of files containing documents separated by blank lines to read.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>list[bytes]</code>           \u2013            <p>List of documents where each document is the read bytestring.</p> </li> </ul> Source code in <code>llm/preprocess/format.py</code> <pre><code>def read_documents_bytes(\n    files: Iterable[pathlib.Path | str] | pathlib.Path | str,\n) -&gt; list[bytes]:\n    \"\"\"Read documents from files.\n\n    Args:\n        files: List of files containing documents separated by blank lines\n            to read.\n\n    Returns:\n        List of documents where each document is the read bytestring.\n    \"\"\"\n    if not isinstance(files, Iterable):\n        files = [files]\n\n    documents: list[bytes] = []\n    document_lines: list[bytes] = []\n\n    for current_file in files:\n        with open(current_file, 'rb') as f:\n            for line_ in f.readlines():\n                line = line_.strip()\n                if len(line) == 0 and len(document_lines) &gt; 0:\n                    documents.append(b'\\n'.join(document_lines))\n                    document_lines = []\n                elif len(line) &gt; 0:\n                    document_lines.append(line)\n\n    if len(document_lines) &gt; 0:\n        documents.append(b'\\n'.join(document_lines))\n\n    return documents\n</code></pre>"},{"location":"api/preprocess/format/#llm.preprocess.format.write_documents","title":"write_documents","text":"<pre><code>write_documents(\n    path: Path | str, documents: list[str]\n) -&gt; None\n</code></pre> <p>Write a list of documents to a file.</p> <p>Parameters:</p> <ul> <li> <code>path</code>               (<code>Path | str</code>)           \u2013            <p>Path to write documents to.</p> </li> <li> <code>documents</code>               (<code>list[str]</code>)           \u2013            <p>Documents to write. Each document will be separated by a blank line.</p> </li> </ul> Source code in <code>llm/preprocess/format.py</code> <pre><code>def write_documents(path: pathlib.Path | str, documents: list[str]) -&gt; None:\n    \"\"\"Write a list of documents to a file.\n\n    Args:\n        path: Path to write documents to.\n        documents: Documents to write. Each document will be separated by\n            a blank line.\n    \"\"\"\n    path = pathlib.Path(path)\n    path.parent.mkdir(exist_ok=True)\n\n    sent_tokenizer = get_sent_tokenizer()\n\n    with open(path, 'w') as f:\n        for document_ in documents:\n            document = document_.replace('\\n', ' ')\n            sentences = sent_tokenizer(document)\n            f.write('\\n'.join(sentences))\n            f.write('\\n\\n')\n</code></pre>"},{"location":"api/preprocess/roberta/","title":"llm.preprocess.roberta","text":"<code>llm/preprocess/roberta.py</code> <p>RoBERTa pretraining encoder.</p> <p>This implements the DOC-SENTENCES sampling strategy of RoBERTa. Samples are not pre-masked as in Devlin et al. and are rather dynamically masked at runtime as in RoBERTa. Next sentence prediction is also not used.</p> <pre><code>python -m llm.preprocess.roberta --help\n</code></pre>"},{"location":"api/preprocess/roberta/#llm.preprocess.roberta.cli","title":"cli","text":"<pre><code>cli(\n    inputs: tuple[str],\n    output_dir: str,\n    tokenizer: str,\n    max_seq_len: int,\n    short_seq_prob: float,\n    processes: int,\n    log_level: str,\n    rich: bool,\n) -&gt; None\n</code></pre> <p>Encode FILEPATHS for RoBERTa pretraining.</p> Source code in <code>llm/preprocess/roberta.py</code> <pre><code>@click.command()\n@click.argument(\n    'inputs',\n    metavar='FILEPATHS',\n    required=True,\n    nargs=-1,\n)\n@click.option(\n    '-o',\n    '--output-dir',\n    metavar='PATH',\n    required=True,\n    help='Output directory for encoded shards.',\n)\n@click.option(\n    '-t',\n    '--tokenizer',\n    metavar='PATH',\n    required=True,\n    help='Path to trained tokenizer to load.',\n)\n@click.option(\n    '-l',\n    '--max-seq-len',\n    type=int,\n    default=512,\n    help='Maximum sequence length.',\n)\n@click.option(\n    '-s',\n    '--short-seq-prob',\n    type=float,\n    default=0.1,\n    help='Probablity to create shorter sequences.',\n)\n@click.option(\n    '-p',\n    '--processes',\n    type=int,\n    default=4,\n    help='Number of processes for concurrent shard encoding.',\n)\n@click.option(\n    '--log-level',\n    default='INFO',\n    type=click.Choice(\n        ['DEBUG', 'INFO', 'WARNING', 'ERROR'],\n        case_sensitive=False,\n    ),\n    help='Minimum logging level.',\n)\n@click.option(\n    '--rich/--no-rich',\n    default=False,\n    help='Use rich output formatting.',\n)\ndef cli(\n    inputs: tuple[str],\n    output_dir: str,\n    tokenizer: str,\n    max_seq_len: int,\n    short_seq_prob: float,\n    processes: int,\n    log_level: str,\n    rich: bool,\n) -&gt; None:\n    \"\"\"Encode FILEPATHS for RoBERTa pretraining.\"\"\"\n    init_logging(log_level, rich=rich)\n\n    # This silences the error:\n    #\n    # The current process just got forked, after parallelism has already been\n    # used. Disabling parallelism to avoid deadlocks...\n    # To disable this warning, you can either:\n    #   - Avoid using `tokenizers` before the fork if possible\n    #   - Explicitly set the environment variable TOKENIZERS_PARALLELISM=false\n    #\n    # Note we set this in a few places to ensure the environment variable\n    # is set in subprocesses.\n    os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n\n    encode_files(\n        input_files=inputs,\n        output_dir=output_dir,\n        tokenizer=tokenizer,\n        max_seq_len=max_seq_len,\n        short_seq_prob=short_seq_prob,\n        processes=processes,\n    )\n</code></pre>"},{"location":"api/preprocess/shard/","title":"llm.preprocess.shard","text":"<code>llm/preprocess/shard.py</code> <p>Pretraining test sharder.</p> <pre><code>python -m llm.preprocess.shard --help\n</code></pre>"},{"location":"api/preprocess/shard/#llm.preprocess.shard.cli","title":"cli","text":"<pre><code>cli(\n    inputs: tuple[str],\n    output_dir: str,\n    size: str,\n    format: str,\n    shuffle: bool,\n    log_level: str,\n    rich: bool,\n) -&gt; None\n</code></pre> <p>Shard documents in FILEPATHS into equally sized files.</p> Source code in <code>llm/preprocess/shard.py</code> <pre><code>@click.command()\n@click.argument(\n    'inputs',\n    metavar='FILEPATHS',\n    required=True,\n    nargs=-1,\n)\n@click.option(\n    '-o',\n    '--output-dir',\n    metavar='PATH',\n    required=True,\n    help='Output directory for encoded shards.',\n)\n@click.option(\n    '-s',\n    '--size',\n    metavar='SIZE',\n    required=True,\n    help='Max data size of each shard.',\n)\n@click.option(\n    '-f',\n    '--format',\n    default='shard-{index}.txt',\n    help='Shard name format where {index} is replaced by shard index.',\n)\n@click.option(\n    '--shuffle/--no-shuffle',\n    default=False,\n    help='Shuffle documents before sharding.',\n)\n@click.option(\n    '--log-level',\n    default='INFO',\n    type=click.Choice(\n        ['DEBUG', 'INFO', 'WARNING', 'ERROR'],\n        case_sensitive=False,\n    ),\n    help='Minimum logging level.',\n)\n@click.option(\n    '--rich/--no-rich',\n    default=False,\n    help='Use rich output formatting.',\n)\ndef cli(\n    inputs: tuple[str],\n    output_dir: str,\n    size: str,\n    format: str,  # noqa: A002\n    shuffle: bool,\n    log_level: str,\n    rich: bool,\n) -&gt; None:\n    \"\"\"Shard documents in FILEPATHS into equally sized files.\"\"\"\n    init_logging(log_level, rich=rich)\n\n    size_bytes = readable_to_bytes(size)\n\n    shard(inputs, output_dir, format, size_bytes, shuffle)\n</code></pre>"},{"location":"api/preprocess/tokenizer/","title":"llm.preprocess.tokenizer","text":"<code>llm/preprocess/tokenizer.py</code> <p>Pretraining tokenizer trainer.</p> <pre><code>python -m llm.preprocess.tokenizer --help\n</code></pre>"},{"location":"api/preprocess/tokenizer/#llm.preprocess.tokenizer.get_tokenizer","title":"get_tokenizer","text":"<pre><code>get_tokenizer(\n    kind: Literal[\"wordpiece\", \"bpe\"],\n    vocab: dict[str, int] | str | None = None,\n    lowercase: bool = False,\n    padding_options: dict[str, Any] | None = None,\n    truncation_options: dict[str, Any] | None = None,\n    **kwargs: Any\n) -&gt; BaseTokenizer\n</code></pre> <p>Get a tokenizer by name.</p> <p>Parameters:</p> <ul> <li> <code>kind</code>               (<code>Literal['wordpiece', 'bpe']</code>)           \u2013            <p>Tokenizer name to create.</p> </li> <li> <code>vocab</code>               (<code>dict[str, int] | str | None</code>, default:                   <code>None</code> )           \u2013            <p>Vocabulary file or dictionary to initialized tokenizer with.</p> </li> <li> <code>lowercase</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Set the tokenizer to lowercase.</p> </li> <li> <code>padding_options</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Enable padding options on the tokenizer.</p> </li> <li> <code>truncation_options</code>               (<code>dict[str, Any] | None</code>, default:                   <code>None</code> )           \u2013            <p>Enable truncation options on the tokenizer.</p> </li> <li> <code>kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>Additional arguments to pass to the tokenizer.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>BaseTokenizer</code>           \u2013            <p>A tokenizer instance.</p> </li> </ul> Source code in <code>llm/preprocess/tokenizer.py</code> <pre><code>def get_tokenizer(\n    kind: Literal['wordpiece', 'bpe'],\n    vocab: dict[str, int] | str | None = None,\n    lowercase: bool = False,\n    padding_options: dict[str, Any] | None = None,\n    truncation_options: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; tokenizers.implementations.base_tokenizer.BaseTokenizer:\n    \"\"\"Get a tokenizer by name.\n\n    Args:\n        kind: Tokenizer name to create.\n        vocab: Vocabulary file or dictionary to initialized tokenizer with.\n        lowercase: Set the tokenizer to lowercase.\n        padding_options: Enable padding options on the tokenizer.\n        truncation_options: Enable truncation options on the tokenizer.\n        kwargs: Additional arguments to pass to the tokenizer.\n\n    Returns:\n        A tokenizer instance.\n    \"\"\"\n    if kind == 'wordpiece':\n        tokenizer = tokenizers.BertWordPieceTokenizer(\n            vocab=vocab,\n            clean_text=True,\n            handle_chinese_chars=True,\n            lowercase=lowercase,\n            **kwargs,\n        )\n    elif kind == 'bpe':\n        tokenizer = tokenizers.ByteLevelBPETokenizer(\n            vocab=vocab,\n            lowercase=lowercase,\n            add_prefix_space=True,\n            trim_offsets=True,\n            **kwargs,\n        )\n    else:\n        raise AssertionError(f'Unsupported kind \"{kind}.\"')\n\n    if padding_options is not None:\n        tokenizer.enable_padding(**padding_options)\n    if truncation_options is not None:\n        tokenizer.enable_truncation(**truncation_options)\n\n    return tokenizer\n</code></pre>"},{"location":"api/preprocess/tokenizer/#llm.preprocess.tokenizer.cli","title":"cli","text":"<pre><code>cli(\n    inputs: tuple[str],\n    output_file: str,\n    size: int,\n    tokenizer: Literal[\"bpe\", \"wordpiece\"],\n    cased: bool,\n    special_token: list[str],\n    log_level: str,\n    rich: bool,\n) -&gt; None\n</code></pre> <p>Train a tokenizer on FILEPATHS.</p> <p>Arguments default to the standard uncased BERT with wordpiece method.</p> Source code in <code>llm/preprocess/tokenizer.py</code> <pre><code>@click.command()\n@click.argument(\n    'inputs',\n    metavar='FILEPATHS',\n    required=True,\n    nargs=-1,\n)\n@click.option(\n    '-o',\n    '--output-file',\n    metavar='PATH',\n    required=True,\n    help='Output file to save serialized tokenizer to.',\n)\n@click.option(\n    '-s',\n    '--size',\n    default=30522,\n    type=int,\n    help='Size of vocabulary.',\n)\n@click.option(\n    '-t',\n    '--tokenizer',\n    type=click.Choice(['bpe', 'wordpiece'], case_sensitive=False),\n    default='wordpiece',\n    help='Tokenizer type.',\n)\n@click.option(\n    '--cased/--uncased',\n    default=False,\n    help='Vocab/tokenizer is case-sensitive.',\n)\n@click.option(\n    '-s',\n    '--special-token',\n    default=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'],\n    metavar='TOKEN',\n    multiple=True,\n    help='Special tokens to prepend to vocab.',\n)\n@click.option(\n    '--log-level',\n    default='INFO',\n    type=click.Choice(\n        ['DEBUG', 'INFO', 'WARNING', 'ERROR'],\n        case_sensitive=False,\n    ),\n    help='Minimum logging level.',\n)\n@click.option(\n    '--rich/--no-rich',\n    default=False,\n    help='Use rich output formatting.',\n)\ndef cli(\n    inputs: tuple[str],\n    output_file: str,\n    size: int,\n    tokenizer: Literal['bpe', 'wordpiece'],\n    cased: bool,\n    special_token: list[str],\n    log_level: str,\n    rich: bool,\n) -&gt; None:\n    \"\"\"Train a tokenizer on FILEPATHS.\n\n    Arguments default to the standard uncased BERT with wordpiece method.\n    \"\"\"\n    init_logging(log_level, rich=rich)\n\n    train_vocab(\n        tokenizer,\n        input_files=inputs,\n        output_file=output_file,\n        size=size,\n        lowercase=not cased,\n        special_tokens=list(special_token),\n    )\n</code></pre>"},{"location":"api/preprocess/utils/","title":"llm.preprocess.utils","text":"<code>llm/preprocess/utils.py</code> <p>Preprocessing script utilities.</p>"},{"location":"api/preprocess/utils/#llm.preprocess.utils.readable_to_bytes","title":"readable_to_bytes","text":"<pre><code>readable_to_bytes(size: str) -&gt; int\n</code></pre> <p>Convert string with bytes units to the integer value of bytes.</p> <p>Source: ProxyStore</p> Example <pre><code>&gt;&gt;&gt; readable_to_bytes('1.2 KB')\n1200\n&gt;&gt;&gt; readable_to_bytes('0.6 MiB')\n629146\n</code></pre> <p>Parameters:</p> <ul> <li> <code>size</code>               (<code>str</code>)           \u2013            <p>String to parse for bytes size.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Integer number of bytes parsed from the string.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>             \u2013            <p>If the input string contains more than two parts (i.e., a value and a unit).</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the unit is not one of KB, MB, GB, TB, KiB, MiB, GiB, or TiB.</p> </li> <li> <code>ValueError</code>             \u2013            <p>If the value cannot be cast to a float.</p> </li> </ul> Source code in <code>llm/preprocess/utils.py</code> <pre><code>def readable_to_bytes(size: str) -&gt; int:\n    \"\"\"Convert string with bytes units to the integer value of bytes.\n\n    Source: [ProxyStore](https://github.com/proxystore/proxystore/blob/79dfdc0fc2c5a5093cf5e57b2ecf61c48fde6773/proxystore/utils.py#L130){target=_blank}\n\n    Example:\n        ```python\n        &gt;&gt;&gt; readable_to_bytes('1.2 KB')\n        1200\n        &gt;&gt;&gt; readable_to_bytes('0.6 MiB')\n        629146\n        ```\n\n    Args:\n        size: String to parse for bytes size.\n\n    Returns:\n        Integer number of bytes parsed from the string.\n\n    Raises:\n        ValueError: If the input string contains more than two parts\n            (i.e., a value and a unit).\n        ValueError: If the unit is not one of KB, MB, GB, TB, KiB, MiB, GiB,\n            or TiB.\n        ValueError: If the value cannot be cast to a float.\n    \"\"\"\n    units_to_bytes = dict(\n        b=1,\n        kb=int(1e3),\n        mb=int(1e6),\n        gb=int(1e9),\n        tb=int(1e12),\n        kib=(2**10),\n        mib=(2**20),\n        gib=(2**30),\n        tib=(2**40),\n    )\n\n    # Try casting size to value (will only work if no units)\n    try:\n        return int(float(size))\n    except ValueError:\n        pass\n\n    # Ensure space between value and unit\n    size = re.sub(r'([a-zA-Z]+)', r' \\1', size.strip())\n\n    parts = [s.strip() for s in size.split()]\n    if len(parts) != 2:\n        raise ValueError(\n            'Input string \"{size}\" must contain only a value and a unit.',\n        )\n\n    value, unit = parts\n\n    try:\n        value_size = decimal.Decimal(value)\n    except decimal.InvalidOperation as e:\n        raise ValueError(f'Unable to interpret \"{value}\" as a float.') from e\n    try:\n        unit_size = units_to_bytes[unit.lower()]\n    except KeyError as e:\n        raise ValueError(f'Unknown unit type {unit}.') from e\n\n    return int(value_size * unit_size)\n</code></pre>"},{"location":"api/preprocess/utils/#llm.preprocess.utils.safe_extract","title":"safe_extract","text":"<pre><code>safe_extract(name: Path | str, target: Path | str) -&gt; None\n</code></pre> <p>Safely extract a tar file.</p> Note <p>This extraction method is designed to safeguard against CVE-2007-4559.</p> <p>Parameters:</p> <ul> <li> <code>name</code>               (<code>Path | str</code>)           \u2013            <p>Path to tar file to extract.</p> </li> <li> <code>target</code>               (<code>Path | str</code>)           \u2013            <p>Target path to extract to.</p> </li> </ul> Source code in <code>llm/preprocess/utils.py</code> <pre><code>def safe_extract(name: pathlib.Path | str, target: pathlib.Path | str) -&gt; None:\n    \"\"\"Safely extract a tar file.\n\n    Note:\n        This extraction method is designed to safeguard against CVE-2007-4559.\n\n    Args:\n        name: Path to tar file to extract.\n        target: Target path to extract to.\n    \"\"\"\n    name = pathlib.Path(name).resolve().absolute()\n    target = pathlib.Path(target).resolve().absolute()\n\n    with tarfile.open(name, 'r:gz') as f:\n        # CVE-2007-4559 input sanitation\n        for member in f.getmembers():\n            member_path = (target / member.name).resolve()\n            if target not in member_path.parents:\n                raise OSError(\n                    'Tarfile contains member that extracts outside of the '\n                    'target directory. This could be a path traversal attack.',\n                )\n        f.extractall(target)\n</code></pre>"},{"location":"api/trainers/","title":"llm.trainers","text":"<code>llm/trainers/__init__.py</code> <p>Training scripts.</p> <p>Each submodule of <code>llm.trainers</code> is an executable module. For example, <code>llm.trainers.bert</code> can be run as: <pre><code>python -m llm.trainers.bert --help\n</code></pre></p>"},{"location":"api/trainers/bert/","title":"llm.trainers.bert","text":"<code>llm/trainers/bert/__init__.py</code> <p>BERT pretraining module.</p> <p>See the BERT Pretraining Guide.</p>"},{"location":"api/trainers/bert/convert/","title":"llm.trainers.bert.convert","text":"<code>llm/trainers/bert/convert.py</code> <p>Convert pretraining checkpoints to HuggingFace pretrained models.</p> Example <p>After training, use the CLI to convert the latest checkpoint. Note that this uses the same configuration file used for training. <pre><code>python -m llm.trainers.bert.convert --config /path/to/config.py --model-path /path/to/output/dir\n</code></pre> Then, use <code>AutoModel.from_pretrained</code> to load the model back. Learn more about AutoModels here.</p> <pre><code>from transformers import AutoModelForMaskedLM\n\nmodel = AutoModelForMaskedLM.from_pretrained('/path/to/output/dir')\n</code></pre>"},{"location":"api/trainers/bert/data/","title":"llm.trainers.bert.data","text":"<code>llm/trainers/bert/data.py</code>"},{"location":"api/trainers/bert/data/#llm.trainers.bert.data.NvidiaBertDatasetConfig","title":"NvidiaBertDatasetConfig","text":"<p>               Bases: <code>NamedTuple</code></p> <p>NVIDIA BERT pretraining dataset configuration.</p>"},{"location":"api/trainers/bert/data/#llm.trainers.bert.data.RobertaDatasetConfig","title":"RobertaDatasetConfig","text":"<p>               Bases: <code>NamedTuple</code></p> <p>RoBERTa pretraining dataset configuration.</p>"},{"location":"api/trainers/bert/data/#llm.trainers.bert.data.get_dataloader","title":"get_dataloader","text":"<pre><code>get_dataloader(\n    dataset: DistributedShardedDataset[Sample],\n    sampler: Sampler[int],\n    batch_size: int,\n) -&gt; DataLoader[Sample]\n</code></pre> <p>Create a dataloader from a sharded dataset.</p> Source code in <code>llm/trainers/bert/data.py</code> <pre><code>def get_dataloader(\n    dataset: DistributedShardedDataset[Sample],\n    sampler: torch.utils.data.Sampler[int],\n    batch_size: int,\n) -&gt; torch.utils.data.DataLoader[Sample]:\n    \"\"\"Create a dataloader from a sharded dataset.\"\"\"\n    return torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        sampler=sampler,\n        num_workers=4,\n        pin_memory=True,\n        drop_last=True,\n    )\n</code></pre>"},{"location":"api/trainers/bert/data/#llm.trainers.bert.data.get_dataset","title":"get_dataset","text":"<pre><code>get_dataset(\n    config: NvidiaBertDatasetConfig | RobertaDatasetConfig,\n) -&gt; DistributedShardedDataset[Sample]\n</code></pre> <p>Load a sharded BERT pretraining dataset.</p> Source code in <code>llm/trainers/bert/data.py</code> <pre><code>def get_dataset(\n    config: NvidiaBertDatasetConfig | RobertaDatasetConfig,\n) -&gt; DistributedShardedDataset[Sample]:\n    \"\"\"Load a sharded BERT pretraining dataset.\"\"\"\n    files = get_filepaths(\n        config.input_dir,\n        extensions=['.h5', '.hdf5'],\n        recursive=True,\n    )\n\n    dataset = torch.utils.data.Dataset\n    params: dict[str, DatasetParams]\n    if isinstance(config, NvidiaBertDatasetConfig):\n        dataset = NvidiaBertDataset\n        params = {file: ((file,), {}) for file in files}\n    elif isinstance(config, RobertaDatasetConfig):\n        tokenizer = tokenizers.Tokenizer.from_file(config.tokenizer_file)\n\n        dataset = RoBERTaDataset\n        kwargs = {\n            'mask_token_id': tokenizer.token_to_id('[MASK]'),\n            'mask_token_prob': config.mask_token_prob,\n            'vocab_size': tokenizer.get_vocab_size(),\n        }\n        params = {file: ((file,), kwargs) for file in files}\n    else:\n        raise AssertionError('Unreachable.')\n\n    return DistributedShardedDataset(\n        dataset,\n        params,\n        rank=torch.distributed.get_rank(),\n        world_size=torch.distributed.get_world_size(),\n    )\n</code></pre>"},{"location":"api/trainers/bert/main/","title":"llm.trainers.bert.main","text":"<code>llm/trainers/bert/main.py</code> <p>BERT pretraining CLI.</p>"},{"location":"api/trainers/bert/utils/","title":"llm.trainers.bert.utils","text":"<code>llm/trainers/bert/utils.py</code> <p>BERT pretraining utilities.</p>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.TrainingConfig","title":"TrainingConfig  <code>dataclass</code>","text":"<pre><code>TrainingConfig(\n    PHASE: int,\n    BERT_CONFIG: dict[str, Any],\n    OPTIMIZER: Literal[\"adam\", \"lamb\"],\n    CHECKPOINT_DIR: str,\n    TENSORBOARD_DIR: str,\n    DATASET_CONFIG: Union[\n        NvidiaBertDatasetConfig, RobertaDatasetConfig\n    ],\n    GLOBAL_BATCH_SIZE: int,\n    BATCH_SIZE: int,\n    STEPS: int,\n    CHECKPOINT_STEPS: int,\n    LR: float,\n    WARMUP_STEPS: int,\n    ACCUMULATION_STEPS: int,\n    CLIP_GRAD_NORM: Optional[float] = None,\n    DTYPE: Optional[dtype] = None,\n    GRADIENT_CHECKPOINTING: bool = False,\n    LOG_FILE: Optional[str] = None,\n    SEED: int = 42,\n)\n</code></pre> <p>Training configuration.</p>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.parse_config","title":"parse_config","text":"<pre><code>parse_config(config: Config) -&gt; TrainingConfig\n</code></pre> <p>Parses a config ensuring all required options are present.</p> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def parse_config(config: Config) -&gt; TrainingConfig:\n    \"\"\"Parses a config ensuring all required options are present.\"\"\"\n    config.ACCUMULATION_STEPS = gradient_accumulation_steps(\n        global_batch_size=config.GLOBAL_BATCH_SIZE,\n        local_batch_size=config.BATCH_SIZE,\n        world_size=torch.distributed.get_world_size(),\n    )\n\n    for field_name, field_type in get_type_hints(TrainingConfig).items():\n        if field_name not in config:\n            continue\n\n        try:\n            match = isinstance(config[field_name], field_type)\n        except TypeError as e:\n            # Not all types (GenericAlias types like dict[str, Any]) will\n            # support isinstance checks so we just log the error and skip\n            match = True\n            logger.debug(\n                f'Unable to verify config option {field_name}: {field_type}\\n'\n                f'{e}',\n            )\n\n        if not match:\n            raise TypeError(\n                f'Expected config entry {field_name} to be type '\n                f'{field_type} but got {type(config[field_name])}.',\n            )\n\n    # Only take args that are fields of TrainingConfig\n    fields = {field.name for field in dataclasses.fields(TrainingConfig)}\n    config_ = {k: v for k, v in config.items() if k in fields}\n\n    return TrainingConfig(**config_)\n</code></pre>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.checkpoint","title":"checkpoint","text":"<pre><code>checkpoint(\n    config: TrainingConfig,\n    global_step: int,\n    epoch: int,\n    model: Module,\n    optimizer: Optimizer,\n    scheduler: _LRScheduler,\n    sampler_index: int = 0,\n) -&gt; None\n</code></pre> <p>Write a training checkpoint.</p> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def checkpoint(\n    config: TrainingConfig,\n    global_step: int,\n    epoch: int,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer,\n    scheduler: torch.optim.lr_scheduler._LRScheduler,\n    sampler_index: int = 0,\n) -&gt; None:\n    \"\"\"Write a training checkpoint.\"\"\"\n    if torch.distributed.get_rank() == 0:\n        # Extract from possible AMPModel\n        model = model._model if hasattr(model, '_model') else model\n        # Extract from possible DistributedDataParallel\n        model = model.module if hasattr(model, 'module') else model\n\n        save_checkpoint(\n            checkpoint_dir=config.CHECKPOINT_DIR,\n            global_step=global_step,\n            model=model,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            epoch=epoch,\n            phase=config.PHASE,\n            sampler_index=sampler_index,\n        )\n    logger.info(\n        f'Saved checkpoint at global step {global_step}',\n        extra={'ranks': [0]},\n    )\n</code></pre>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.load_state","title":"load_state","text":"<pre><code>load_state(\n    config: TrainingConfig,\n    model: Module,\n    optimizer: Optimizer | None = None,\n    scheduler: _LRScheduler | None = None,\n) -&gt; tuple[int, int, int]\n</code></pre> <p>Load the latest checkpoint if one exists.</p> <p>Returns:</p> <ul> <li> <code>int</code>           \u2013            <p>Tuple of the global step, epoch, and sampler index to resume</p> </li> <li> <code>int</code>           \u2013            <p>(or start) from.</p> </li> </ul> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def load_state(\n    config: TrainingConfig,\n    model: torch.nn.Module,\n    optimizer: torch.optim.Optimizer | None = None,\n    scheduler: torch.optim.lr_scheduler._LRScheduler | None = None,\n) -&gt; tuple[int, int, int]:\n    \"\"\"Load the latest checkpoint if one exists.\n\n    Returns:\n        Tuple of the global step, epoch, and sampler index to resume\n        (or start) from.\n    \"\"\"\n    global_step = 0\n    epoch = 1\n    sampler_index = 0\n\n    os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)\n    checkpoint = load_checkpoint(\n        config.CHECKPOINT_DIR,\n        map_location='cpu',  # next(model.parameters()).device,\n    )\n    if checkpoint is None:\n        logger.info(\n            'No checkpoint found to resume from',\n            extra={'ranks': [0]},\n        )\n        return global_step, epoch, sampler_index\n\n    logger.info(\n        f'Loaded checkpoint from {checkpoint.filepath}',\n        extra={'ranks': [0]},\n    )\n    # Load model to the model and not the AMP wrapper\n    model = model._model if hasattr(model, '_model') else model\n    # Load model to the model and not the DDP wrapper\n    model = model.module if hasattr(model, 'module') else model\n    model.load_state_dict(checkpoint.model_state_dict)\n\n    if checkpoint.kwargs['phase'] == config.PHASE:\n        logger.info(\n            'Checkpoint from current phase. Loading training state',\n            extra={'ranks': [0]},\n        )\n\n        if (  # pragma: no branch\n            checkpoint.optimizer_state_dict is not None\n            and optimizer is not None\n        ):\n            optimizer.load_state_dict(checkpoint.optimizer_state_dict)\n\n        if (  # pragma: no branch\n            checkpoint.scheduler_state_dict is not None\n            and scheduler is not None\n        ):\n            scheduler.load_state_dict(checkpoint.scheduler_state_dict)\n\n        global_step = checkpoint.global_step\n        epoch = checkpoint.kwargs['epoch']\n        sampler_index = (\n            checkpoint.kwargs['sampler_index']\n            if 'sampler_index' in checkpoint.kwargs\n            else 0\n        )\n    else:\n        logger.info(\n            'Checkpoint from new phase. Resetting training state',\n            extra={'ranks': [0]},\n        )\n\n    return global_step, epoch, sampler_index\n</code></pre>"},{"location":"api/trainers/bert/utils/#llm.trainers.bert.utils.get_optimizer_grouped_parameters","title":"get_optimizer_grouped_parameters","text":"<pre><code>get_optimizer_grouped_parameters(\n    model: BertForPreTraining,\n) -&gt; list[dict[str, Any]]\n</code></pre> <p>Get the parameters of the BERT model to optimizer.</p> Source code in <code>llm/trainers/bert/utils.py</code> <pre><code>def get_optimizer_grouped_parameters(\n    model: transformers.BertForPreTraining,\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Get the parameters of the BERT model to optimizer.\"\"\"\n    # configure the weight decay for bert models\n    params = list(model.named_parameters())\n    no_decay = ['bias', 'gamma', 'beta', 'LayerNorm']\n\n    params_decay = [p for n, p in params if not any(v in n for v in no_decay)]\n    params_no_decay = [p for n, p in params if any(v in n for v in no_decay)]\n\n    return [\n        {'params': params_decay, 'weight_decay': 0.01},\n        {'params': params_no_decay, 'weight_decay': 0.0},\n    ]\n</code></pre>"},{"location":"api/trainers/gpt/","title":"llm.trainers.gpt","text":"<code>llm/trainers/gpt/__init__.py</code> <p>GPT pretraining module.</p> <p>See the GPT Pretraining Guide.</p>"},{"location":"api/trainers/gpt/arguments/","title":"llm.trainers.gpt.arguments","text":"<code>llm/trainers/gpt/arguments.py</code> <p>GPT trainer argument parser.</p>"},{"location":"api/trainers/gpt/data/","title":"llm.trainers.gpt.data","text":"<code>llm/trainers/gpt/data.py</code>"},{"location":"api/trainers/gpt/data/#llm.trainers.gpt.data.get_datasets","title":"get_datasets","text":"<pre><code>get_datasets(\n    *,\n    dataset_name: str | None = None,\n    dataset_config_name: str | None = None,\n    validation_split_percentage: float = 0,\n    train_file: str | None = None,\n    validation_file: str | None = None,\n    keep_linebreaks: bool = True\n) -&gt; Dataset | DatasetDict\n</code></pre> <p>Get the datasets.</p> <p>You can either provide your own CSV/JSON/TXT training and evaluation files (see below) or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/ (the dataset will be downloaded automatically from the datasets Hub).</p> <p>For CSV/JSON files, this script will use the column called 'text' or the first column if no column called 'text' is found. You can easily tweak this behavior (see below).</p> <p>In distributed training, the load_dataset function guarantee that only one local process can concurrently.</p> Source code in <code>llm/trainers/gpt/data.py</code> <pre><code>def get_datasets(\n    *,\n    dataset_name: str | None = None,\n    dataset_config_name: str | None = None,\n    validation_split_percentage: float = 0,\n    train_file: str | None = None,\n    validation_file: str | None = None,\n    keep_linebreaks: bool = True,\n) -&gt; datasets.Dataset | datasets.DatasetDict:\n    \"\"\"Get the datasets.\n\n    You can either provide your own CSV/JSON/TXT training and evaluation files\n    (see below) or just provide the name of one of the public datasets\n    available on the hub at https://huggingface.co/datasets/ (the dataset will\n    be downloaded automatically from the datasets Hub).\n\n    For CSV/JSON files, this script will use the column called 'text' or the\n    first column if no column called 'text' is found. You can easily tweak this\n    behavior (see below).\n\n    In distributed training, the load_dataset function guarantee that only one\n    local process can concurrently.\n    \"\"\"\n    if dataset_name is not None:\n        # Downloading and loading a dataset from the hub.\n        raw_datasets = datasets.load_dataset(dataset_name, dataset_config_name)\n        if 'validation' not in raw_datasets.keys():\n            raw_datasets['validation'] = datasets.load_dataset(\n                dataset_name,\n                dataset_config_name,\n                split=f'train[:{validation_split_percentage}%]',\n            )\n            raw_datasets['train'] = datasets.load_dataset(\n                dataset_name,\n                dataset_config_name,\n                split=f'train[{validation_split_percentage}%:]',\n            )\n    elif train_file is not None:\n        data_files = {}\n        dataset_args = {}\n        data_files['train'] = train_file\n        if validation_file is not None:\n            data_files['validation'] = validation_file\n        extension = train_file.split('.')[-1]\n        if extension == 'txt':\n            extension = 'text'\n            dataset_args['keep_linebreaks'] = keep_linebreaks\n        raw_datasets = datasets.load_dataset(\n            extension,\n            data_files=data_files,\n            **dataset_args,\n        )\n        # If no validation data is there, validation_split_percentage will be\n        # used to divide the dataset.\n        if 'validation' not in raw_datasets.keys():\n            raw_datasets['validation'] = datasets.load_dataset(\n                extension,\n                data_files=data_files,\n                split=f'train[:{validation_split_percentage}%]',\n                **dataset_args,\n            )\n            raw_datasets['train'] = datasets.load_dataset(\n                extension,\n                data_files=data_files,\n                split=f'train[{validation_split_percentage}%:]',\n                **dataset_args,\n            )\n    else:\n        raise ValueError('One of dataset_name or train_file must be provided.')\n\n    return raw_datasets\n</code></pre>"},{"location":"api/trainers/gpt/data/#llm.trainers.gpt.data.preprocess_datasets","title":"preprocess_datasets","text":"<pre><code>preprocess_datasets(\n    *,\n    raw_datasets: DatasetT,\n    tokenizer: AutoTokenizer,\n    accelerator: Accelerator,\n    num_workers: int | None = None,\n    overwrite_cache: bool = False,\n    block_size: int | None = None\n) -&gt; DatasetT\n</code></pre> <p>Preprocessing the datasets.</p> Source code in <code>llm/trainers/gpt/data.py</code> <pre><code>def preprocess_datasets(\n    *,\n    raw_datasets: DatasetT,\n    tokenizer: transformers.AutoTokenizer,\n    accelerator: accelerate.Accelerator,\n    num_workers: int | None = None,\n    overwrite_cache: bool = False,\n    block_size: int | None = None,\n) -&gt; DatasetT:\n    \"\"\"Preprocessing the datasets.\"\"\"\n    # First we tokenize all the texts.\n    column_names = raw_datasets['train'].column_names\n    text_column_name = 'text' if 'text' in column_names else column_names[0]\n\n    def tokenize_function(examples: dict[str, Any]) -&gt; Any:\n        return tokenizer(examples[text_column_name])\n\n    with accelerator.main_process_first():\n        tokenized_datasets = raw_datasets.map(\n            tokenize_function,\n            batched=True,\n            num_proc=num_workers,\n            remove_columns=column_names,\n            load_from_cache_file=not overwrite_cache,\n            desc='Running tokenizer on dataset',\n        )\n\n    if block_size is None:\n        block_size = tokenizer.model_max_length\n        if block_size &gt; 1024:\n            logger.warning(\n                'The chosen tokenizer supports a `model_max_length` that is '\n                'longer than the default `block_size` value of 1024. If you '\n                'would like to use a longer `block_size` up to '\n                '`tokenizer.model_max_length` you can override this default '\n                'with `--block_size xxx`.',\n                extra={'ranks': [0]},\n            )\n        block_size = 1024\n    else:\n        if block_size &gt; tokenizer.model_max_length:\n            logger.warning(\n                f'The block_size passed ({block_size}) is larger than '\n                'the maximum length for the model'\n                f'({tokenizer.model_max_length}). '\n                f'Using block_size={tokenizer.model_max_length}.',\n                extra={'ranks': [0]},\n            )\n        block_size = min(block_size, tokenizer.model_max_length)\n\n    assert isinstance(block_size, int)\n\n    # Note that with `batched=True`, this map processes 1,000 texts together,\n    # so group_texts throws away a remainder for each of those groups of\n    # 1,000 texts. You can adjust that batch_size here but a higher value\n    # might be slower to preprocess.\n    #\n    # To speed up this part, we use multiprocessing. See the documentation of\n    # the map method for more information:\n    # https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map  # noqa: E501\n    with accelerator.main_process_first():\n        lm_datasets = tokenized_datasets.map(\n            functools.partial(group_texts, block_size=block_size),\n            batched=True,\n            num_proc=num_workers,\n            load_from_cache_file=not overwrite_cache,\n            desc=f'Grouping texts in chunks of {block_size}',\n        )\n\n    return lm_datasets\n</code></pre>"},{"location":"api/trainers/gpt/data/#llm.trainers.gpt.data.group_texts","title":"group_texts","text":"<pre><code>group_texts(\n    examples: dict[str, Any], block_size: int\n) -&gt; dict[str, Any]\n</code></pre> <p>Concatenates texts from dataset and generates chunks of block_size.</p> Source code in <code>llm/trainers/gpt/data.py</code> <pre><code>def group_texts(examples: dict[str, Any], block_size: int) -&gt; dict[str, Any]:\n    \"\"\"Concatenates texts from dataset and generates chunks of block_size.\"\"\"\n    # Concatenate all texts.\n    concatenated_examples = {\n        k: list(itertools.chain(*examples[k])) for k in examples.keys()\n    }\n    total_length = len(concatenated_examples[next(iter(examples.keys()))])\n    # We drop the small remainder, and if the total_length &lt; block_size we\n    # exclude this batch and return an empty dict. We could add padding if the\n    # model supported it instead of this drop, you can customize this part to\n    # your needs.\n    total_length = (total_length // block_size) * block_size\n    # Split by chunks of max_len.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result['labels'] = result['input_ids'].copy()\n    return result\n</code></pre>"},{"location":"api/trainers/gpt/main/","title":"llm.trainers.gpt.main","text":"<code>llm/trainers/gpt/main.py</code> <p>GPT pretraining CLI.</p> <p>Script modified from https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm_no_trainer.py.</p>"},{"location":"api/trainers/gpt/model/","title":"llm.trainers.gpt.model","text":"<code>llm/trainers/gpt/model.py</code>"},{"location":"api/trainers/gpt/model/#llm.trainers.gpt.model.load_model","title":"load_model","text":"<pre><code>load_model(\n    *,\n    config_name: str | None = None,\n    model_name_or_path: str | None = None,\n    model_type: str | None = None,\n    tokenizer_name: str | None = None,\n    use_slow_tokenizer: bool = False,\n    low_cpu_mem_usage: bool = False\n) -&gt; tuple[PreTrainedModel, PreTrainedTokenizer]\n</code></pre> <p>Load pretrained model and tokenizer.</p> <p>In distributed training, the <code>.from_pretrained</code> methods guarantee that only one local process can concurrently download model &amp; vocab.</p> Source code in <code>llm/trainers/gpt/model.py</code> <pre><code>def load_model(\n    *,\n    config_name: str | None = None,\n    model_name_or_path: str | None = None,\n    model_type: str | None = None,\n    tokenizer_name: str | None = None,\n    use_slow_tokenizer: bool = False,\n    low_cpu_mem_usage: bool = False,\n) -&gt; tuple[transformers.PreTrainedModel, transformers.PreTrainedTokenizer]:\n    \"\"\"Load pretrained model and tokenizer.\n\n    In distributed training, the `.from_pretrained` methods guarantee that only\n    one local process can concurrently download model &amp; vocab.\n    \"\"\"\n    if config_name is not None:\n        config = AutoConfig.from_pretrained(config_name)\n    elif model_name_or_path is not None:\n        config = AutoConfig.from_pretrained(model_name_or_path)\n    else:\n        config = CONFIG_MAPPING[model_type]()\n        logger.warning(\n            'You are instantiating a new config instance from scratch.',\n            extra={'ranks': [0]},\n        )\n\n    if tokenizer_name:\n        tokenizer = AutoTokenizer.from_pretrained(\n            tokenizer_name,\n            use_fast=not use_slow_tokenizer,\n        )\n    elif model_name_or_path:\n        tokenizer = AutoTokenizer.from_pretrained(\n            model_name_or_path,\n            use_fast=not use_slow_tokenizer,\n        )\n    else:\n        raise ValueError(\n            'You are instantiating a new tokenizer from scratch. This is not '\n            'supported by this script. You can do it from another script, '\n            'save it, and load it from here, using --tokenizer_name.',\n        )\n\n    if model_name_or_path:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name_or_path,\n            from_tf=bool('.ckpt' in model_name_or_path),\n            config=config,\n            low_cpu_mem_usage=low_cpu_mem_usage,\n        )\n    else:\n        logger.info('Training new model from scratch', extra={'ranks': [0]})\n        model = AutoModelForCausalLM.from_config(config)\n\n    # We resize the embeddings only when necessary to avoid index errors. If\n    # you are creating a model from scratch on a small vocab and want a smaller\n    # embedding size, remove this test.\n    embedding_size = model.get_input_embeddings().weight.shape[0]\n    if len(tokenizer) &gt; embedding_size:\n        model.resize_token_embeddings(len(tokenizer))\n\n    return model, tokenizer\n</code></pre>"},{"location":"api/trainers/gpt/optimizer/","title":"llm.trainers.gpt.optimizer","text":"<code>llm/trainers/gpt/optimizer.py</code>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#getting-started-for-local-development","title":"Getting Started for Local Development","text":"<p>We recommend using Tox to setup the development environment. This will create a new virtual environment with all of the required packages installed and <code>llm</code> installed in editable mode with the necessary extras options.</p> <pre><code>$ git clone https://github.com/gpauloski/llm-pytorch\n$ cd llm-pytorch\n$ tox --devenv venv -e py310\n$ . venv/bin/activate\n</code></pre> <p>Warning</p> <p>Running Tox in a Conda environment is possible but it may conflict with Tox's ability to find the correct Python versions. E.g., if your Conda environment is Python 3.9, running <code>$ tox -e p38</code> may still use Python 3.9.</p> <p>To install manually: <pre><code>$ git clone https://github.com/gpauloski/llm-pytorch\n$ cd llm-pytorch\n$ python -m venv venv\n$ . venv/bin/activate\n$ pip install -e .[dev,docs,colossalai]\n</code></pre></p>"},{"location":"contributing/#continuous-integration","title":"Continuous Integration","text":"<p>This package uses pre-commit and Tox for continuous integration (test, linting, etc.).</p>"},{"location":"contributing/#linting-and-type-checking-pre-commit","title":"Linting and Type Checking (pre-commit)","text":"<p>To use pre-commit, install the hook and then run against files.</p> <pre><code>$ pre-commit install\n$ pre-commit run --all-files\n</code></pre>"},{"location":"contributing/#tests-tox","title":"Tests (tox)","text":"<p>The entire CI workflow can be run with <code>$ tox</code>. This will test against multiple versions of Python and can be slow.</p> <p>Module-level unit-test are located in the <code>tests/</code> directory and its structure is intended to match that of <code>llm/</code>. E.g. the tests for <code>llm/preprocess/shard.py</code> are located in <code>tests/preprocess/shard.py</code>; however, additional test files can be added as needed. Tests should be narrowly focused and target a single aspect of the code's functionality, tests should not test internal implementation details of the code, and tests should not be dependent on the order in which they are run.</p> <p>Code that is useful for building tests but is not a test itself belongs in the <code>testing/</code> directory.</p> <pre><code># Run all tests in tests/\n$ tox -e py39\n# Run a specific test\n$ tox -e py39 -- tests/preprocess/shard.py::test_cli\n</code></pre>"},{"location":"contributing/#docs","title":"Docs","text":"<p>If code changes require an update to the documentation (e.g., for function signature changes, new modules, etc.), the documentation can be built using MKDocs.</p> <pre><code># With tox (will only build, does not serve)\n$ tox -e docs\n\n# Or manually\n$ pip install -e .[docs]\n$ mkdocs build --strict  # Build only to site/index.html\n$ mkdocs serve           # Serve locally\n</code></pre> <p>Docstrings are automatically generated, but it is recommended to check the generated docstrings to make sure details/links/etc. are correct.</p>"},{"location":"contributing/issues-pull-requests/","title":"Issues and Pull Requests","text":""},{"location":"contributing/issues-pull-requests/#issues","title":"Issues","text":"<p>Issue Tracker</p> <p>We use GitHub issues to report problems, request and track changes, and discuss future ideas. If you open an issue for a specific problem, please follow the template guides.</p>"},{"location":"contributing/issues-pull-requests/#pull-requests","title":"Pull Requests","text":"<p>We use the standard GitHub contribution cycle where all contributions are made via pull requests (including code owners!).</p> <ol> <li>Fork the repository and clone to your local machine.</li> <li>Create local changes.<ul> <li>Changes should conform to the style and testing guidelines, referenced   above.</li> <li>Preferred commit message format (source):<ul> <li>separate subject from body with a blank line,</li> <li>limit subject line to 50 characters,</li> <li>capitalize first word of subject line,</li> <li>do not end the subject line with a period,</li> <li>use the imperative mood for subject lines,</li> <li>include related issue numbers at end of subject line,</li> <li>wrap body at 72 characters, and</li> <li>use the body to explain what/why rather than how.   Example: <code>Fix concurrency bug in XYZ (#42)</code></li> </ul> </li> </ul> </li> <li>Push commits to your fork.<ul> <li>Please squash commits fixing mistakes to keep the git history clean.   For example, if commit \"b\" follows commit \"a\" and only fixes a small typo   from \"a\", please squash \"a\" and \"b\" into a single, correct commit.   This keeps the commit history readable and easier to search through when   debugging (e.g., git blame/bisect).</li> </ul> </li> <li>Open a pull request in this repository.<ul> <li>The pull request should include a description of the motivation for the   PR and included changes. A PR template is provided to guide this process.</li> </ul> </li> </ol>"},{"location":"contributing/style-guide/","title":"Style Guide","text":"<p>The Python code and docstring format mostly follows Google's Python Style Guide, but the pre-commit config is the authoritative source for code format compliance.</p> <p>Nits:</p> <ul> <li>Avoid imports in <code>__init__.py</code> (reduces the likelihood of circular imports).</li> <li>Prefer pure functions where possible.</li> <li>Define all class attributes inside <code>__init__</code> so all attributes are visible   in one place. Attributes that are defined later can be set as <code>None</code>   as a placeholder.</li> <li>Prefer f-strings (<code>f'name: {name}</code>) over string format   (<code>'name: {}'.format(name)</code>). Never use the <code>%</code> operator.</li> <li>Prefer typing.NamedTuple over collections.namedtuple.</li> <li>Exception messages should read as complete sentences with punctuation.   Logging messages can forgo trailing punctuation.   <pre><code>raise ValueError('Name must contain alphanumeric characters only.')\nlogger.info(f'New connection opened to {address}')\n</code></pre></li> <li>Document all exceptions that may be raised by a function in the docstring.</li> </ul>"},{"location":"guides/","title":"Guides","text":"<p>See the sidebar for the list of available guides.</p>"},{"location":"guides/bert-pretraining/","title":"BERT Pretraining","text":"<p>This guide walks through BERT pretraining based on NVIDIA's configuration. This configuration uses large batch training with LAMB to achieve 64K phase 1 and 32K phase 2 batch sizes.</p>"},{"location":"guides/bert-pretraining/#setup","title":"Setup","text":"<p>This guide assumes you have installed the <code>llm</code> packages and its dependencies as described in the Installation Guide. You also need the BERT pretraining dataset from NVIDIA's examples.</p> <p>We will use the configuration is provided in configs/bert-large-lamb.py. We start by copying the example configuration into our training directory. Note all of the commands are from the root of the <code>llm-pytorch</code> directory. <pre><code>mkdir -p runs/bert-large-pretraining/\ncp configs/bert-large-lamb.py runs/bert-large-pretraining/config.py\n</code></pre> You can inspect <code>runs/bert-large-pretraining/config.py</code> to see if you want to adjust any options, though the default paths will work.</p>"},{"location":"guides/bert-pretraining/#running-the-trainer","title":"Running the Trainer","text":"<p>There are a number of ways you may launch the trainer:</p> <ul> <li>Single-GPU for Debugging: <pre><code>python -m llm.trainers.bert --config path/to/config.py --debug\n</code></pre></li> <li>Multi-GPU Single-Node: <pre><code>torchrun --nnodes=1 --nproc_per_node=auto --standalone \\\n    -m llm.trainers.bert --config path/to/config.py\n</code></pre></li> <li>Multi-Node Multi-GPU: <pre><code>torchrun --nnodes $NNODES --nproc_per_node=auto --max_restarts 0 \\\n    --rdzv_backend=c10d --rdzv_endpoint=$PRIMARY_RANK \\\n    -m llm.trainers.bert --config path/to/config.py\n</code></pre></li> </ul> <p>Typically, you will want to use the last option inside of the script you submit to your batch scheduling system. This is an example submission script for a PBS scheduler. <pre><code>#!/bin/bash\n#PBS -A __ALLOCATION__\n#PBS -q __QUEUE__\n#PBS -M __EMAIL__\n#PBS -m abe\n#PBS -l select=16:system=polaris\n#PBS -l walltime=6:00:00\n#PBS -l filesystems=home:grand\n#PBS -j oe\n\n# Figure out training environment based on PBS_NODEFILE existence\nif [[ -z \"${PBS_NODEFILE}\" ]]; then\n    RANKS=$HOSTNAME\n    NNODES=1\nelse\n    PRIMARY_RANK=$(head -n 1 $PBS_NODEFILE)\n    RANKS=$(tr '\\n' ' ' &lt; $PBS_NODEFILE)\n    NNODES=$(&lt; $PBS_NODEFILE wc -l)\n    cat $PBS_NODEFILE\nfi\n\nCONFIG=\"runs/bert-large-pretraining/config.py\"\n\n# Commands to run prior to the Python script for setting up the environment\nmodule load cray-python\nmodule load cudatoolkit-standalone/11.7.1\nsource /path/to/virtualenv\n\n# torchrun launch configuration\nLAUNCHER=\"torchrun \"\nLAUNCHER+=\"--nnodes=$NNODES --nproc_per_node=auto --max_restarts 0 \"\nif [[ \"$NNODES\" -eq 1 ]]; then\n    LAUNCHER+=\"--standalone \"\nelse\n    LAUNCHER+=\"--rdzv_backend=c10d --rdzv_endpoint=$PRIMARY_RANK\"\nfi\n\n# Training script and parameters\nCMD=\"$LAUNCHER -m llm.trainers.bert --config $CONFIG\"\necho \"Training Command: $CMD\"\n\nmpiexec --hostfile $PBS_NODEFILE -np $NNODES --env OMP_NUM_THREADS=8 --cpu-bind none $CMD\n</code></pre></p>"},{"location":"guides/bert-pretraining/#training-phase-1-and-2","title":"Training Phase 1 and 2","text":"<p>Train the model for phase 1. After the end of phase 1, you'll see a checkpoint named <code>runs/bert-large-pretraining/checkpoints/phase-1/global_step_7039.pt</code>.</p> <p>To transition to phase 2, set <code>PHASE = 2</code> in the config file. Then create a new directory for phase 2 checkpoints at <code>runs/bert-large-pretraining/checkpoints/phase-2</code>. Copy the last checkpoint from phase 1 to the phase 2 directory with the name <code>global_step_0.pt</code>. Continue training to complete phase 2/</p>"},{"location":"guides/bert-pretraining/#converting-the-pretrained-model","title":"Converting the Pretrained Model","text":"<p>TODO</p>"},{"location":"guides/bert-pretraining/#squad-evaluation","title":"SQUAD Evaluation","text":"<p>TODO</p>"},{"location":"guides/gpt-pretraining/","title":"GPT Pretraining","text":"<p>This guide walks you through pretraining GPT-like causal language model. Instructions are specific to ALCF's Polaris machine; however, the general steps should apply for any system.</p> <p>The training script is based on HuggingFace's CLM example.</p>"},{"location":"guides/gpt-pretraining/#installation","title":"Installation","text":"<ol> <li>Clone the repository.    <pre><code>$ git clone https://github.com/gpauloski/llm-pytorch\n$ cd llm-pytorch\n</code></pre></li> <li>Load the Python and CUDA modules on Polaris. These modules will need to be    loaded each time you activate the virtual environment.    <pre><code>$ module load cray-python/3.9.12.1\n$ module load cudatoolkit-standalone/11.7.1\n</code></pre></li> <li>Create a virtual environment.    <pre><code>$ python -m venv venv\n$ . venv/bin/activate\n</code></pre></li> <li>Install PyTorch and the <code>llm-pytorch</code> package. Other versions of PyTorch    should work fine. I have personally tests PyTorch 1.13.1 with CUDA 11.7    on Polaris.    <pre><code>$ pip install torch==1.13.1+cu117 --extra-index-url https://download.pytorch.org/whl/cu117\n$ pip install -e .[kfac]\n</code></pre></li> </ol>"},{"location":"guides/gpt-pretraining/#running-the-scripts","title":"Running the Scripts","text":"<p>The basic training command is <code>python -m llm.trainers.gpt {options}</code>. The script uses HuggingFace Accelerate to detect the distributed environment. I suggest using <code>torchrun</code> for distributed training. E.g.,</p> <pre><code>torchrun --nnodes=1 --nproc_per_node=auto --standalone -m llm.trainers.gpt {options}\n</code></pre> <p>Here is an example job script for Polaris (using PBS) which will automatically set up the distributed environment according to your job parameters. Note that this example trains a small 125M parameter GPT model on WikiText. Highlighted lines contain information that you must complete yourself.</p> pretrain.pbs<pre><code>#!/bin/bash\n#PBS -A __ALLOCATION__\n#PBS -q __QUEUE__\n#PBS -M __EMAIL__\n#PBS -m abe\n#PBS -l select=16:system=polaris\n#PBS -l walltime=6:00:00\n#PBS -l filesystems=home:grand\n#PBS -j oe\n\n# This stores the list of command line arguments passed to llm.trainers.gpt.\n# WARNING: this is not an exhaustive list of options. See:\n#     python -m llm.trainers.gpt --help\nOPTIONS=\"--low_cpu_mem_usage \"\n\n# Dataset options\nOPTIONS+=\"--dataset_name wikitext \"\nOPTIONS+=\"--dataset_config_name wikitext-2-raw-v1 \"\n\n# Model options\nOPTIONS+=\"--model_name_or_path EleutherAI/gpt-neo-125m \"\n\n# Logging/checkpointing options\nOPTIONS+=\"--output_dir runs/gpt-neo-125m-pretraining \"\nOPTIONS+=\"--checkpointing_steps 1000 \"\nOPTIONS+=\"--resume_from_checkpoint \"\n\n# Training parameters\nOPTIONS+=\"--max_train_steps 10000 \"\nOPTIONS+=\"--per_device_train_batch_size 1 \"\nOPTIONS+=\"--per_device_eval_batch_size 1 \"\nOPTIONS+=\"--gradient_accumulation_steps 8 \"\nOPTIONS+=\"--mixed_precision fp16 \"\n\n# KFAC options\n# OPTIONS+=\"--kfac \"\n# OPTIONS+=\"--kfac-factor-update-steps 5 \"\n# OPTIONS+=\"--kfac-inv-update-steps 50 \"\n\n# Commands to run prior to the Python script for setting up the environment\nmodule load cray-python\nmodule load cudatoolkit-standalone/11.7.1\nsource /path/to/virtualenv\n\n# Figure out training environment based on PBS_NODEFILE existence\nif [[ -z \"${PBS_NODEFILE}\" ]]; then\n    RANKS=$HOSTNAME\n    NNODES=1\nelse\n    PRIMARY_RANK=$(head -n 1 $PBS_NODEFILE)\n    RANKS=$(tr '\\n' ' ' &lt; $PBS_NODEFILE)\n    NNODES=$(&lt; $PBS_NODEFILE wc -l)\n    cat $PBS_NODEFILE\nfi\n\n# torchrun launch configuration\nLAUNCHER=\"torchrun \"\nLAUNCHER+=\"--nnodes=$NNODES --nproc_per_node=auto --max_restarts 0 \"\nif [[ \"$NNODES\" -eq 1 ]]; then\n    LAUNCHER+=\"--standalone \"\nelse\n    LAUNCHER+=\"--rdzv_backend=c10d --rdzv_endpoint=$PRIMARY_RANK\"\nfi\n\n# Training script and parameters\nCMD=\"$LAUNCHER -m llm.trainers.gpt $OPTIONS\"\necho \"Training Command: $CMD\"\n\nmpiexec --hostfile $PBS_NODEFILE -np $NNODES --env OMP_NUM_THREADS=8 --cpu-bind none $CMD\n</code></pre> <p>After updating <code>pretrain.pbs</code> accordingly, you can either execute the script directly in an interactive session or submit it as a batch job.</p> <p>Interactive: <pre><code>$ qsub -A {ALLOC} -l select=1:system=polaris -l walltime=1:00:00 -I -q debug -l filesystems=home:grand\n$ chmod u+x pretrain.pbs\n$ ./pretrain.pbs\n</code></pre> Batch: <pre><code>$ qsub pretrain.pbs\n</code></pre></p>"},{"location":"guides/gpt-pretraining/#customize-pretraining","title":"Customize Pretraining","text":""},{"location":"guides/gpt-pretraining/#model","title":"Model","text":"<p>This script uses HuggingFace models. The argument <code>--model_name_or_path</code> takes either a path to a saved HuggingFace model directory or the name of a model on the HuggingFace Hub.</p> <p>Here's some useful options:</p> <ul> <li><code>--model_name_or_path EleutherAI/gpt-neo-125m</code>: Small GPT model useful for debugging.</li> <li><code>--model_name_or_path EleutherAI/gpt-neo-1.3B</code>: Works with K-FAC and is almost the same size as GPT-2.</li> <li><code>--model_name_or_path EleutherAI/gpt-neox-20b</code>: GPT NeoX 20B.</li> <li><code>--model_name_or_path gpt2</code>: HuggingFace's GPT-2 implementation which uses Conv1D layers instead of Linear layers so does not work with K-FAC.</li> </ul> <p>Note that the <code>--low_cpu_mem_usage</code> option will instantiate the model architecture for pretraining without downloading the actual weights.</p> <p>Alternatively, a <code>--config_name</code> and <code>--tokenizer_name</code> can be provided where each can either be a name of an existing model/tokenizer or a path to their respective HuggingFace compatible configurations.</p>"},{"location":"guides/gpt-pretraining/#dataset","title":"Dataset","text":"<p>There are two ways to provide a pretraining dataset: via HuggingFace Datasets or CSV/JSON/text files.</p> <p>To use an existing dataset via the Dataset Hub, find the name of the dataset and the name of the subset. <pre><code># Generic format\n--dataset_name {name} --dataset_config_name {subset}\n# WikiText (good for testing)\n--dataset_name wikitext --dataset_config_name wikitext-2-raw-v1\n# The Pile\n--dataset_name EleutherAI/pile --dataset_config_name all\n# The Pile-10K (subset for testing)\n--dataset_name NeelNanda/pile-10k\n</code></pre></p> <p>Datasets are downloaded to <code>~/.cache/huggingface/datasets</code>. This can be changed by setting <code>HF_DATASETS_CACHE</code>. <pre><code>$ export HF_DATASETS_CACHE=\"/path/to/another/directory\"\n</code></pre></p>"},{"location":"guides/gpt-pretraining/#checkpointing","title":"Checkpointing","text":"<p>Checkpointing is not enabled by default. Use <code>--checkpointing_steps {STEPS}</code> to enable checkpointing. To resume training from a checkpoint, add <code>--resume_from_checkpoint</code>.</p>"},{"location":"guides/gpt-pretraining/#limitations","title":"Limitations","text":"<ul> <li>FP16 training with HuggingFace Accelerate is faster than FP32 but still   uses the same amount of memory.</li> </ul>"},{"location":"guides/roberta-preprocessing/","title":"RoBERTa Pretraining Preprocessing Guide","text":"<p>This guide walks through downloading, formatting, and encoding the Wikipedia and books corpora for pretraining RoBERTa.</p> <p>This guide assumes you have installed the <code>llm</code> packages and its dependencies as described in the Installation Guide.</p> <p>Note: using both the Wikipedia and books corpora is not necessary, and you can skip one or the other. These instructions will also work for your own text corpora---just skip the download step.</p>"},{"location":"guides/roberta-preprocessing/#download","title":"Download","text":"<p>Create a <code>datasets/</code> directory. This directory will contain all of the files produced. <pre><code>$ mkdir datasets/\n</code></pre></p> <p>Download the datasets. <pre><code>$ python -m llm.preprocess.download --dataset wikipedia --output-dir datasets/downloaded/\n$ python -m llm.preprocess.download --dataset bookscorpus --output-dir datasets/downloaded/\n</code></pre> This will result in two files: <code>datasets/downloaded/wikipedia-{date}.en.txt</code> and <code>datasets/downloaded/bookcorpus.txt</code>. Each of these files has the format one sentence per line with documents separated by blank lines.</p>"},{"location":"guides/roberta-preprocessing/#shard","title":"Shard","text":"<p>Next we shard the files to make them easier to work with. For small dataset this may not be needed.</p> <pre><code>$ python -m llm.preprocess.shard datasets/downloaded/*.txt --output-dir datasets/sharded/wikibooks/ --size 250MB\n</code></pre> <p>Now we have a set of sharded files in <code>datasets/sharded/wikibooks/</code> that are each approximately 250 MB. The format of these files is still one sentence per line with documents separated by blank lines.</p>"},{"location":"guides/roberta-preprocessing/#tokenzier","title":"Tokenzier","text":"<p>Now we train a tokenizer on the text corpus. BPE and wordpiece tokenizers are supported as well as cased/uncased. This example creates an uncased wordpiece tokenizer will 50,000 tokens.</p> <pre><code>$ python -m llm.preprocess.tokenizer datasets/sharded/wikibooks/* \\\n      --output-file datasets/tokenizers/wikibooks-wordpiece.json \\\n      --tokenizer wordpiece \\\n      --size 50000 \\\n      --uncased\n</code></pre> <p>The resulting JSON file can be loaded to get the trained tokenizer. <pre><code>from tokenizers import Tokenizer\n\ntokenizer = Tokenizer.from_file('datasets/tokenizers/wikibooks-wordpiece.json')\n</code></pre></p>"},{"location":"guides/roberta-preprocessing/#encode-the-shards","title":"Encode the Shards","text":"<p>Now we can encode the shards using our tokenizer.</p> <pre><code>$ python -m llm.preprocess.roberta datasets/sharded/wikibooks/* \\\n      --output-dir datasets/encoded/wikibooks/ \\\n      --tokenizer datasets/tokenizers/wikibooks-wordpiece.json \\\n      --max-seq-len 512 \\\n      --processes 4\n</code></pre> <p>Encoding a 250MB shard can take around 16GB of RAM so adjust the number of processes (parallel workers encoding a shard) as needed to fit your shard size and system memory.</p> <p>This produces a set of encoded HDF5 files in <code>datasets/encoded/wikibooks</code> with each encoded file corresponding to a shard. Each encoded file contains the <code>input_ids</code>, <code>attention_masks</code>, and <code>special_tokens_masks</code> attributes which are each numpy arrays of shape <code>(samples, max_seq_len)</code>.</p> <p>In contrast to BERT encoding, the samples are not pre masked and next sentence prediction is not done. In other words, masking must be done at runtime and each sample represents contiguous sentences draw from the same document until the max sequence length is reached.</p>"},{"location":"installation/","title":"Installation","text":"<p>This package is Linux only and requires Python &gt;=3.9. It is recommended to install the package in a virtual environment of your choice. <pre><code>$ python -m venv venv     # or $ virtualenv venv\n$ . venv/bin/activate\n$ pip install torch       # torch install instructions may differ\n$ pip install .           # use -e for editable mode\n</code></pre> PyTorch installation instructions vary by system and CUDA versions so check the latest instructions here.</p> <p>ColossalAI can be installed to use the <code>FusedAdam</code> and <code>FusedLAMB</code> optimizers. See the directions here.</p>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>Development installation instructions are provided in the Contributing Guide.</p>"},{"location":"installation/#system-specific-installation","title":"System Specific Installation","text":"<p>Below are installation guides for specific HPC systems.</p> <ul> <li>Polaris</li> </ul>"},{"location":"installation/polaris/","title":"Polaris Installation","text":"<ol> <li>Load Polaris modules.    <pre><code>module load cray-python/3.9.12.1\nmodule load cudatoolkit-standalone/11.7.1\n</code></pre></li> <li>Clone and create your virtual environment.    <pre><code>git clone git@github.com:gpauloski/llm-pytorch\n# git clone https://github.com/gpauloski/llm-pytorch\ncd llm-pytorch\n</code></pre></li> <li>Create your virtual environment.    <pre><code>python -m venv venv\n. venv/bin/activate\n</code></pre> Note: anytime you want to use the virtual environment you will need to    load the above module and activate the virtual environment. It may be    helpful to add these to your <code>~/.bashrc</code> or PBS job scripts.</li> <li>Install PyTorch.    <pre><code>pip install torch==1.13.1\n</code></pre></li> <li>Install the <code>llm</code> package.    <pre><code>pip install -e .  # Use the [dev] extras install for developing\n</code></pre></li> <li>Install ColossalAI.    This can be done directly from pip (<code>pip install colossalai</code>) which will    JIT compile the CUDA extensions, or you can install with CUDA extensions    pre-build.    This must be done from a compute node.    E.g., <code>qsub -A [ALLOCATION] -l select=1:system=polaris -l walltime=1:00:00 -I -q debug -l filesystems=home:grand</code>.    <pre><code># Activate your modules and virtual environment\nmodule load cray-python/3.9.12.1 cudatoolkit-standalone/11.7.1\n. /path/to/venv/bin/activate\n\nmodule load gcc\n\nCUDA_EXT=1 pip install colossalai\n</code></pre></li> <li>Done!</li> </ol>"}]}